\documentclass{article}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsmath}
% \usepackage{amsfonts}

\usepackage{xcolor}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}
\newcommand{\spencer}[1]{{\color{blue} Spencer: #1}}

\begin{document}
  
  
\section*{Review BA2412-010}
\subsection*{Overall assessment:}
This paper by Wadsworth and Niemi adapts two models originally designed to forecast outpatient
ILI data in order to apply them to influenza hospitalization data. This is motivated by a switch in the
target definition of the US CDC FluSight challenge. The paper is well-written and pleasant to read.
The overall idea of leveraging models from previous FluSight cycles for the new target is appealing.
However, several points require clarification. Given the focus of the journal, the Bayesian inference
aspect should be developed more thoroughly as it currently seems a little ad hoc. Moreover, it
currently does not become clear to which degree the proposed methods offer improved forecasts
compared to existing (and potentially much simpler) models. I provide some specific comments in
the following.


\subsection*{Specific comments:}

\begin{enumerate}[1.]
\item The entire Bayesian inference scheme seems somewhat heuristic.

\begin{enumerate}[a.]
\item The ILI model is run separately and then feeds into the hospitalization model. So there is no
flow of information from the hospitalization data to the ILI model, which there would be in a fully
Bayesian procedure. Is this done on purpose and if so what is the motivation? The proposed
procedure reminds me of the "Bayesian cut" in the version described by Plummer
\href{(https://link.springer.com/article/10.1007/s11222-014-9503-z)}{(https://link.springer.com/article/10.1007/s11222-014-9503-z)}.
This may be a useful reference.

\spencer{We appreciated this comment and the included reference to the Bayesian
cut. This led to several new insights. 
We added a couple paragraphs of discussion to address this. They appear in
the revised manuscript as below.

``This two-component modeling is remeniscent of the "Bayesian cut" as 
described in [44] and [41]. Cutting a model
into two component models or ``modules`` may be justfied for several reasons
including to reduce time to fit the model, avoiding mixing issues of posterior
sampling distributions, improving predictive performance or to prevent
model misspecification in one model to influence the other 
[41, 27, 26, 44].

The decision to cut the forecast model into two component models
was made for several reasons.
When fitting the fully Bayesian joint model, the time to fit a single model using
Markov chain Monte Carlo (MCMC) sampling and
obtain forecasts was sometimes many hours whereas fitting the two models
separately took a fraction of the time. When assessing convergence of model
parameters, it was rare that posterior sampling chains for the joint model
showed convergence even after 55,000 iterations, whereas the component model 
fits gave no signs which raised concern for convergence. And most importantly,
the forecasts from the component models generally outperformed those of the 
joint model in terms of minimizing proper scoring rules.``}

% \spencer{In the forecasting competition, the data comes generally by the afternoon and
% forecasts are due by the evening. We initially did a full Bayesian procedure
% (minus the MLE selection of the $\lambda$ parameter in the ASG ILI model), but fit 
% was very slow and made it difficutl to get forecasts submitted in time. We switched
% to using two separate models because of the time saved in model fitting. We also
% found that in terms of the proper scoring rules and predictive interval coverage,
% there wasn't a drop in performance although the parameter estimation did 
% change.
% 
% \begin{equation}
% \begin{aligned}
%     p(\tilde{H}_{s,w^*} | \textbf{ILI}, \textbf{H}) &= \int 
%        p(\tilde{H}_{s,w^*} | \textbf{ILI}, \textbf{H}, \boldsymbol{\alpha}) 
%        p(\boldsymbol{\alpha} | \textbf{ILI}, \textbf{H}) d\boldsymbol{\alpha} \\
%     p(\tilde{H}_{s,w^*} | \textbf{ILI}, \textbf{H}, \boldsymbol{\alpha}) &= 
%        \int p(\tilde{H}_{s,w^*} | \textbf{H}, \textbf{ILI}, 
%        \widetilde{ILI}_{s,w^*}, \boldsymbol{\alpha}) 
%        p(\widetilde{ILI}_{s,w^*} | \textbf{ILI}) d\widetilde{ILI}_{s,w^*}
% \end{aligned}
% \end{equation}
% }

\item The authors employ a "modular Bayesian approach" mixing maximum likelihood and
Bayesian estimation. The authors should provide more ample references to the literature concerning
this type of procedure (is "modular Bayesian" a standard term?).

\spencer{In the revised manuscript we no longer use the modular approach
where some parameters are set as their MLEs before fitting a Bayesian 
model. Instead
we assign a strong prior to the parameter which was giving us trouble
and fit a full Bayesian model. This is
described in the following statement added to revised manuscript:

``In a previous version of this manuscript, when fitting an ASG model
with discrepancy components,
rather than estimating all parameters through posterior updating,
identifiability was encouraged by setting
the parameter $\lambda_s$ from (4)
to be equal to its MLE. In this updated manuscript, 
however, we use empirical Bayes and center the first component of the 
hierarichal prior $\theta$ to be the mean of MLEs for $\lambda_s$ across
all seasons and set the variance parameter so the prior was tight around the
mean. This led to improved mixing of posterior draws and better forecasts.``}

\item Also, it should be argued explicitly why this approach was chosen over a standard fully
Bayesian approach in the setting at hand. Was a fully Bayesian approach also explored? Which
difficulties were encountered?

\spencer{See response to comment 1b.}

\item Notably it would be helpful to comment on how this approach affects the uncertainty
propagation / overall quantification of forecast uncertainty.

\spencer{See response to comment 1b.}

\end{enumerate}


\item Several comments on the data and general modelling choices:

\begin{enumerate}[a.]
\item Influenza-like illness (ILI) is not a single pathogen, but a mix of several pathogens causing
similar symptoms. How reasonable is it to assume SIR dynamics for such an indicator? Do we
really assume that the tipping point due to susceptible depletion can be captured reasonably here? In
my understanding this would be the only reason to include an S compartment.

\spencer{Obviously the SIR dynamics are not a perfect match for ILI data, but
the disease transmission mechanism of influenza is at least partially 
responsible for ILI. We touch on this a little more directly in the revised 
manuscript by stating:

``The SIR is a mechanistic model used to capture transmission of a particular
disease, and since ILI may be influenced by more 
diseases than influenza, the disease transmission described by the SIR model
can only partially contribute to the true ILI data-generating mechanism
[44].

Ulloa [53]
chose to use the asymmetric Gaussian (ASG) function to model ILI data.
The ASG is a smooth function which may approximate the 
same shape as the ILI trajectory.
The ASG function has more parameters and is thus slightly more flexible than
the SIR model, and thus may be able to roughly capture ILI.
However, there may still be 
systematic behavior which it cannot capture. Thus when using either the SIR
or ASG models, an additional component for modeling discrepancy should be 
included."

We cite Osthus et al. 2019, who also touch on this.}


\item My impression is that the SIR does not really have any mechanistic meaning here and is just a
way of parameterizing curves that look qualitatively like ILI seasons. The ASG approach can be
seen as an alternative parameterization for such curves. Could the authors comment on what
practical differences exist between the two parameterizations?

\spencer{Practially, the reason to use the SIR model is generally because it is meant
to capture some of the scientific understanding of a disease outbreak. Again, we argue 
that the ASG is a reasonable alternative to SIR models in the case of the seasonal flu
outbreak. The ASG shape with a rise to a peak and a subsequent fall is similar in shape
to the regular ILI trajectory. It has also been used successfully in forecasting of 
other pheneomena with similar rises and falls (i.e. crop moisture)}.



\item Is the hospitalization indicator specific to influenza? My understanding is that it is, but this
should be more clear in Section 2.2. If it is influenza-specific: to which degree should we expect ILI
to be a helpful predictor given that it also contains COVID, RSV etc.? The influenza dynamics are
likely somewhat hidden in the ILI indicator.

\spencer{Yes, the hospitalization indicator is specific to influenza. See 
response to the next comment (2d.) for how it was addressed in the manuscript. 

For your second point on how well ILI can be a helpful predictor of hospitalizations,
we addressed this in the manuscript by adding the following discussion about
figure 6.


``Figure 6 helps
illustrate why this modeling decision was made.
The figure shows scatterplots of ILI percentage and 
hospitalizations on the top row.
The bottom row shows scatterplots of the difference between hospitalizations and 
1 week lags
by ILI percentage at the US national level. The plots in the right column
are of the logarithm of hospitalizations. The lines through the points are
drawn by connecting hospitalizations for consecutive weeks during the flu 
season.
These plots show that a linear or quadratic relationship between ILI percentage 
and hospitalizations does not capture much of the temporal correlation in 
hospitalizations as shown by the obvious loops drawn in the plot. 
However, when including a 
single autoregressive lag of hospitalizations in the model, much of the time 
correlation is
accounted for as shown by the tighter and more frequent crossing of lines
in the bottom row. Models with more than 1 autoregressive
lag showed little or no 
improvement over 1 lag.``}




\item Does the hospitalization indicator refer to incident (new) hospitalization or prevalence, i.e.,
bed occupancy?

\spencer{It refers to new hospitalizations. 
We revised the first paragraph in section 2.2 to read:

``Weekly hospital admission data, which was used as the object of FluSight 
forecasting for the 
2022 and 2023 seasons, is based on the CDC's National Healthcare Safety 
Network (NHSN) dataset entitled \textit{HealthData.gov COVID-19 Reported 
Patient Impact and Hospital Capacity by State Timeseries}. 
In February 2022 it became mandatory for 
all hospitals to report the number of COVID-19 and influenza hospitalizations, 
and since then reporting of hospitalizations has become widespread. These data 
were updated every Wednesday and Friday according to NHSN guidelines 
[22]. Weekly influenza hospitalization data is
defined as the number of newly hospitalized patients with a confirmed diagnosis 
of influenza. From hereon, influenza hospitalizations
will be referrred to as hospitalizations.``}



\item Is there any "mechanistic" rationale behind the ARX formulation for the hospitalizations? If we
predict prevalence / bed occupancy I can sort of see why the differences between weeks would be
explained by ILI. If we predict new hospitalizations I am not sure what motivates this ARX
formulation.

\spencer{ILI is useful as a predictor of hospitalizations, but it fails to 
capture the correlation between consecutive hospitalizations. By adding the 
AR1 component to a model with ILI as a predictor, much of the correlation
structure is then accounted for. 
See the second part of our response to comment 2c. above for how this is 
addressed in the revised manuscript.}

\item Currently the ILI value from the same week is used in equation (7). Would there be a point in
exploring different lags?


\spencer{Exploring different lags is an important idea here, especially considering there
may be a reporting lag. However, the reporting lag is not as big an issue as it has
been in the past. We assessed ILI lags for both one and two weeks previous to the current
week and found that ILI had less predictive power. We note this in the
revised manuscript by saying:

`` We found that models
 with more than 1 autoregressive lag showed 
 little or no forecast improvement over 1 lag.``}

\item On page 14 it says: "all negative values of [...] were set to 0 to reflect realistic values of
hospitalizations". Could this be handled more naturally by appropriate distributional assumptions?

\spencer{We tried both fitting log-hospitalizations as well as assuming 
hospitalizations followed a distribution truncated at 0, but this led to
poorer forecasts or problems fitting the posterior distribution. We make this
statement in the revised manuscript:

``The sample $\{H^*_{s,w + i}\}^K$ was then used as the probabilistic forecast 
for hospitalizations at week $w + i$. 
Occasionally $\{H^*_{s,w + i}\}^K$ included a small number of negative values,
which do not make sense when the distribution is meant to forecast
hospitalizations, a nonnegative number. We attempted to alleviate this problem
by modeling log-hospitalizations or by assuming hospitalizations followed a
distribution truncated at 0, but these models led to poorer forecasts
and issues drawing from the posterior distribution.
For the forecast competition analysis in 
section 5, all negative values of $\{H^*_{s,w + i}\}^K$ were 
set to 0 to reflect realistic values of hospitalizations and comply with the 
FluSight forecasting rules.``}

\end{enumerate}


\item And some remarks on the evaluation.

\begin{enumerate}[a.]

\item It would be interesting to have comparisons of performance beyond the variations of the two
proposed models. There should definitely be some naive baseline model (e.g., a persistence
forecast) and some out-of-the-box model applied directly to the hospitalizations (e.g. seasonal
ARIMA). Given the forecasting task was taken from the CDC challenge it would also be interesting
to see how the proposed models fare against the other participants of the challenge.


\spencer{In the revised manuscript, 
both the simulation study and the real data analysis were updated to include
comparison of our models to others. In the simulation study, we included out 
of the box methods including an ARIMA model and a random walk baseline model
(similar to the baseline used in the FluSight competition).
In the real data analysis, our model forecasts are compared to those of 20
non-ensemble models submitted to the FluSight competition during the 2023-24
season. In both the simulation study and the real data analysis, 
our models show very good performance when compared to the other models. 
The original real data analysis where we compare several models that
belong to our framework is now in the supplementary materials.}



\item The measure the authors call LWIS tends to give a lot of weight to forecast tasks with low
(expected) incidence as the relative differences between predictions and observations can be quite
large then. The "regular" WIS gives most weight to weeks with high incidence. So in a sense both
are complementary and it may be helpful to also discuss regular WIS a little more.

\spencer{In the revised manuscript, we discuss both the WIS and LWIS in the 
real data analysis section and report both measures. The results for both are
very similar, so to save space we focus a bit more on the LWIS and included 
a little more discussion, including a few plots,
on WIS results in the supplementrary materials.}

\item Why was the CRPS used in the simulation and WIS in the data example? These two are sort of
the same (WIS approximates CRPS), so things would be more coherent if one was used throughout.

\spencer{In the revised manuscript we do use the CRPS, but rather 
use the WIS in both the simulation study
and the real data analysis. We decided on the WIS because that was the measure
used in the FluSight competition.}

\item Some assessment of calibration, e.g. interval coverage, should be added.

\spencer{We updated figures 7 and 9, which show results for the simulation
study and the real data analysis respectively, to include a plot showing
empirical predictive coverage for several nominal levels. This showed that 
our forecasts are reasonably well calibrated relative to others which we 
compare them to.}

\end{enumerate}

\item The authors should at least discuss the possibility of sharing certain parameters across geographic
units as done e.g. in
\href{https://www.nature.com/articles/s41467-021-23234-5}{https://www.nature.com/articles/s41467-021-23234-5}
by Osthus and Moran.

\spencer{The following two statements were added. The first in the main body
and the second in the conslusion.

``Model (1) was fit 
independently for each location. This decision was made largely due to the 
limited time given by FluSight to produce forecasts. That is,
during the competition, data were typically released on Wednesday afternoons and
forecasts were due by the evening of the same day giving only a few hours to 
produce forecasts. A natural extension of the model would be to allow for states
to borrow information from neighboring states.``

``The models in this manuscript were such that the data in each
location were modeled as
independent from all other locations. It is reasonable to assume that a
disease outbreak in one state will share characterstics with the outbreak in 
neighboring states, and indeed the behavior of ILI and hospitalizations is more
closely related for neighboring states as shown in data plots in the
supplementary materials. Notably, the one 2023 FluSight forecast model which 
outperformed
the ASGD\_NORM2, the UGA\_flucast-INFLAenza, was listed as being a spatial
model. And in past seasons, some of the top performing forecast models also 
explicitly modeled spatial relationships [43].``}

\end{enumerate}


\section*{Review BA2412-010R1AE1}

The present paper proposes a novel two component framework for projecting Health 
and Human Services (HHS) hospitalization, based on hospitalization and 
influenza-like illness (ILI) data. In a first, step ILI data are modelled which 
are then used as a predictive covariate in the modelling and projection of the 
hospitalisation data. Probabilistic forecasts are assessed in a simulation study 
using proper scoring rules.


\begin{enumerate}[1.]

\item Since this paper is submitted to Bayesian analysis, the authors should 
underline more the modelling aspects and why this is an important contribution 
for the Bayesian community. What is the impact and importance of priors. Does 
the model borrow information between different model parts, propagate 
uncertainty ... In the introduction none of this is mentioned so the question 
arises whether Bayesian Analysis is the right journal.

\spencer{

Statement at the beginning of section 3.

``Because we desire probabilistic forecasts, Bayesian modeling is a 
natural framework for describing forecast uncertainty. 
Though our main interest is in effeciently producing a probabilistic 
forecast and not 
necessarily in estimating model parameters, certain modeling decisions and 
prior distribution selections proved critical in producing good 
forecasts. After defining the ILI and hospitalization models, 
we describe the 
selection of prior distributions, model fitting, and producing forecasts
from the posterior predictive distribution by combining ILI and hospitalization
models in a way similar to using the 
Bayesian cut [46]."


Here's an example of one of a 
few other statements we added which describes the imporance of 
an informative prior for a 
specific parameter.

``Modeling discrepancy can lead to 
overfitting, particularly in forecasting scenarios, and may also lead to 
identifiability issues. 
Thus, care must be taken in setting parameter constraints as well as in the 
selection of prior distributions [42,8]. 
For instance, when setting $f_{\theta_s}(w)$ to
be the ASG function
in model (1), we found the parameter $\lambda$ from 
in (\ref{eq:asg_function_rep}) difficult to consistently 
estimate precisely because of lack of identifiability. This was resolved by 
assigning to $\lambda$ a more informative prior distribution.``}

\item As I understand one challenge of forecasting hospitalisation data is that the 
start of the collection of these data is very recent, so that a model only 
based on these data has not much historic data to be based on. Thus, the idea 
of using ILI data is natural. The authors could emphasize more and discuss 
potential alternative models that could even be included in the simulation 
study. I am missing a comparison to models that are not the newly proposed 
ones within the paper.


\spencer{In the revised manuscript, 
both the simulation study and the real data analysis were updated to include
comparison of our models to others. In the simulation study, we included out 
of the box methods including an ARIMA model and a random walk baseline model
(similar to the baseline used in the FluSight competition).
In the real data analysis, our model forecasts are compared to those of 20
non-ensemble models submitted to the FluSight competition during the 2023-24
season. In both the simulation study and the real data analysis, 
our models show very good performance when compared to the other models. 
The original real data analysis where we compare several models that
belong to our framework is now in the supplementary materials.}

\item Why is it sensible to include ILI as a linear function when modelling 
hospitalisation. Please motivate more. 

\spencer{In the original manuscript, figure 6 and some surrounding discussion
were intended to get across the point that hospitalizations can reasonably
taken as a linear predictor of ILI along with an autoregressive component.
Obviously it didn't get the point across very well. 
In the revised manuscript, figure 6 has been replaced with an updated and more
descriptive figure (also titled "figure 6") along with the in manuscript
text.

``Figure 6 helps
illustrate why this modeling decision was made.
The figure shows scatterplots of ILI percentage and 
hospitalizations on the top row.
The bottom row shows scatterplots of the difference between hospitalizations and 
1 week 
lags
by ILI percentage at the US national level. The plots in the right column
are of the logarithm of hospitalizations. The lines through the points are
drawn by connecting hospitalizations for consecutive weeks during the flu 
season.
These plots show that a linear or quadratic relationship between ILI percentage 
and hospitalizations does not capture much of the temporal correlation in 
hospitalizations as shown by the obvious loops drawn in the plot. 
However, when including a 
single AR lag of hospitalizations in the model, much of the time 
correlation is
accounted for as shown by the tighter and more frequent crossing of lines
in the bottom row. Models with more than 1 AR lag showed little or no 
improvement over 1 lag."}

\item The authors provide also state level predictions. However, I do not see 
where a spatial component is included in the model. Is this only via P or are 
all states just modelled independently. For influenza I would think that it 
might be reasonable to assume that neighbouring states show similar behaviour 
in both ILI and hosptialisation so that borrowing strength between states 
within one model seems sensible. Please discuss.


\spencer{Yes, borrowing strength between states would be reasonable, but 
fitting such a model would likely take much more time than was give by FluSight
to produce forecasts. In the revised manuscript, after defining the ILI model
we state:

``Model (1) was fit 
independently for each location. This decision was made largely due to the 
limited time given by FluSight to produce forecasts. That is,
during the competition, data were typically released on Wednesday afternoons and
forecasts were due by the evening of the same day giving only a few hours to 
produce forecasts. A natural extension of the model would be to allow for states
to borrow information from neighboring states.`` 

We also added the following to the discussion:

``The models in this manuscript were such that the data in each
location were modeled as
independent from all other locations. It is reasonable to assume that a
disease outbreak in one state will share characterstics with the outbreak in 
neighboring states, and indeed the behavior of ILI and hospitalizations is more
closely related for neighboring states as shown in data plots in the
supplementary materials. Notably, the one 2023 FluSight forecast model which 
outperformed
the ASGD\_NORM2, the UGA\_flucast-INFLAenza, was listed as being a spatial
model. And in past seasons, some of the top performing forecast models also 
explicitly modeled spatial relationships [43].``}

\end{enumerate}


\subsection*{Specific comments:}

\begin{enumerate}[-]

\item Section 2, page 3: please be more clear on what you mean by the 
"first seven season". Provide a time frame.

\spencer{The ``first seven seasons" was replaced with 
`` the first seven seasons, 2013 to 2019,".} 

\item Figure 1: I was wondering whether the R-package geofacet could be useful to 
show also a spatial trend across US states, in addition to temporal trend, for 
certain years,  similar to the second plot provided here:

https://hafen.github.io/geofacet/

I could image that the curves, onset and general behaviour, varies across the 
states. The authors refer to this a bit in the discussion. In addition I would 
prefer to also have months names in the axis labels, to avoid confusion that 
week 1 is not the first week in January.

Can the hospitalization figures also be shown on state level at least in an appendix.

\spencer{I made the changes to the x-axes so that they are labeled by date, 
listed as month, rather than by week. I also added in the 
supplementary material the geofacet plots showing 
ILI and hospitalizations for each state organized by geographic location. Some
discussion about the relation between neighboring states is made there.}

\item Section 3.5: Please be clear what the strength of a Bayesian inference scheme is. 

\item Equation 6): What does the tilde mean?

\spencer{In the revised manuscript we state:

``Here $\widetilde{ILI}_{s,w + i}$ represents the 
predicted or forecast ILI at $i$ weeks in the future."}

\item Page 11: Please motivate the choice of an autoregressive process of order 1 
more clearly. Why not use a higher order? Would this be beneficial?


\spencer{We explored higher orders and found that they didn't provide any more
predictive power than order 1 did. See our response to your general comment 3.}

\item Page 14: Please discuss the assumption that all components of $\alpha_s$ and 
$\sigma_\epsilon^2$ are assumed to be same across all seasons. Is this realistic?

\spencer{We do not generally make this assumption in our model. The only time
we make this assumption is when modeling log-hospitalizations instead of 
hospitalizations and then we do it becuase otherwise the predictive variance
was unreasonably large for lack of hospitalization data.
Where this is pertinent in the supplementary materials, we note ??.
In the simulation study,
we simulate hospitalization data which we attempt to forecast for each left
out season in the leave one season out scheme. For clarification, we added 
the following.

``Note that this is not
a model assumption we make but only a simplyfying assumption for simiulating
the data."}

\item Figure 7: Please make clear in the caption what the columns represent in an 
intuitive manner. 

\spencer{In the updated manuscript, this figure was replaced with another figure
which included 3 plots we believe is much more intuitive and descriptive than
the original figure. The title of the new figure remains ``figure 7" and 
the caption is hopefully understandable.}

\item Equation 11, 12, 13, 14: Please motivate. The authors write in Section 3.4 
that the modelling of discrepancy might require careful selection of priors. 
Please underline more how this is taken care of in this manuscript.

\spencer{We decided to put equations 11 - 14 in the supplementary materials, and
give more of an over description of the prior distributions in section 3.6. Here
is one updated paragraph describing how a prior distribution was assigned to
improve identifiability issues introduced by including discprepancy modeling.

``Additional prior constraints were made to improve parameter identifiability. 
Just as was done in Osthus et al. [42],
we set the initial value of the 
susceptible population compartment of the SIR model was set to $S_0 = 0.9$. 
The parameters $I_{0s}$, $\beta_s$, and $\rho_s$ were assigned informative 
priors. In a previous version of this manuscript, when fitting an ASG model
with discrepancy components,
rather than estimating all parameters through posterior updating,
we set the parameter $\lambda_s$ from (4)
to be equal to its MLE. In this updated manuscript, 
however, we use empirical Bayes and center the first component of the 
hierarichal prior $\theta$ to be the mean of MLEs for $\lambda_s$ across
all seasons and set the variance parameter so the prior was tight around the
mean."}

\item Please check for typos.
  
\end{enumerate}
\end{document}
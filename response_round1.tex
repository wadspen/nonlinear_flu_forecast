\documentclass[ba]{imsart}

\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue,backref=page,backref=page]{hyperref}
\RequirePackage{graphicx}


%%%%%%%%%%%packages added by Spencer%%%%%%%%%%%%%
\usepackage{booktabs} %for the table in the analysis section
\usepackage{enumerate} %for the short algorithm
\usepackage{subfigure} %for making plots with multiple images
\usepackage{tabularx} %for the spacing in tabular
\usepackage{newtxmath} %for making indicator function
\usepackage{changepage} %for adjusting the margins
\usepackage{dsfont} %for getting the indicator function
\usepackage{mathtools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\1}[1]{\mathds{1}\left[#1\right]}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{xcolor}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}
\newcommand{\spencer}[1]{{\color{red} Response: #1}}

\begin{document}
\Huge
\noindent Response to reviewers
\\~\\
\normalsize
We are grateful to the three reviewers and their insightful comments and 
questions. Overall, they made a huge difference in improving the content of
our manuscript. In the most recent revision, we are particularly grateful for 
the questions on uncertainty propogation. Obviously the explanation we gave
for how uncertainty propogation wasn't clear enough, so in this iteration of 
 we reworked sections 3.7 and 3.8 to make more clear how 
uncertainty propogation was done. Specifics are found below and in the 
revised manuscript. Reviewer comments are \emph{italicized}, our responses to 
each reviewer's comments are in normal script, 
and specific changes/additions to the main manuscript are 
indented.
\\~\\


\Huge
\noindent AE1
\normalsize
\\~\\

% \emph{
% I think the authors did a great job in this newly submitted version and 
% addressed most of the earlier raised comments in a nice and convincing way. 
% The paper is well written and interesting to read. 
% }
% I only have two remaining comments.

\begin{enumerate}[-]

  \item \emph{Section 3.7. End of of first paragraph: ``..., but propagation of 
  uncertainty   in ILI forecasts was still permitted in the estimating of 
  hospitalisation forecasts.``
  }
  \emph{
  Please clarify more explicitly how uncertainty was propagated between the two 
  model components. My understanding is that this was achieved by incorporating 
  not only point estimates of the parameters but also samples from the predictive 
  distribution of the ILI model. If this is correct, please state it clearly 
  already at this stage and consider adding a brief discussion of the advantages 
  and disadvantages of this approach.}
  \\~\\
  To clarify how uncertainty was propogated, we consolidated sections
  3.7 and 3.8, reordered the main ideas, and removed redundant information. We
  also added some advantages/disadvantages of our approach. The changes now
  read:
  \begin{quote}
        \subsection*{3.7 Hospitalization Forecasts Using the Bayesian Cut}
        \label{sec:implementation_posterior}
        
        Rather than combine the ILI and hopsitalization models into a single model,
        (1) and 
        (6) were fit separately via Bayesian posterior updating 
        in two stages and then fused together to produce hospitalization 
        forecasts. In the first stage, future ILI 
        forecasts were obtained from the posterior predictive distribution where for
        week $w$, the predictive distribution was obtained by integrating over the 
        parameters $\pi_s$, $\kappa_s$, $\sigma^2_{\gamma}$, and $\sigma^2_{\gamma_W}$ 
        as in (\ref{eq:ili_post}) where 
        $p(\boldsymbol{\pi}_s, \kappa_s, \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w)$ 
        is the density function of the posterior distribution for the model parameters
        and $\textbf{ILI}_w$ represents all observed ILI data up to week $w$. 
        The desired forecasts were for weeks $w + i$ 
        where $i$ is a positive integer. Here $\widetilde{ILI}_{s,w + i}$ represents the 
        predicted or forecasted ILI at $i$ weeks in the future.
        
        \begin{equation}
            \label{eq:ili_post}
            \tag{7}
            p(\widetilde{ILI}_{s,w +i} | \textbf{ILI}_w) = \int \int \int \int 
            p(\widetilde{ILI}_{s,w + i} | \boldsymbol{\pi}_s, \kappa_s, 
            \sigma^2_{\gamma}, \sigma^2_{\gamma_W}) p(\boldsymbol{\pi}_s, \kappa_s, 
            \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w) 
            d\boldsymbol{\pi}_s d \kappa_s d \sigma^2_{\gamma} d \sigma^2_{\gamma_W}
        \end{equation}
        % \jarad{It doesn't make sense to have 4 $\int$ here since $\pi_s$ is a vector.}
        % \spencer{Isn't that captured in $d \pi_s$?}
        
        \indent In the second stage, the posterior distribution
        for parameters of model (6) given observed
        hospitalization data $\textbf{H}_w$, observed ILI data, and
        denoted as 
        $p(\boldsymbol{\alpha}_s, \sigma_{\epsilon_s} | \textbf{H}_w, \textbf{ILI}_w)$ 
        was obtained.
        The posterior predictive in (\ref{eq:ili_post}) was then combined with 
        the hospitalization parameter posterior distribution
        to produce a hospitalization forecast $\widetilde{H}_{s,w + i}$ 
        for week $w + i$.
        Both the ILI posterior predictive in (\ref{eq:ili_post}) and the hospitalization
        parameter posterior were estimated via Markov chain Monte Carlo (MCMC) sampling.
        That is, the first stage component model produced $K$ draws
        $(\widetilde{ILI}_{s,w +i}^{(1)}, ..., \widetilde{ILI}_{s,w +i}^{(K)})$ 
        and the second stage component model produced
        $K$ parameter draws 
        $(\boldsymbol{\alpha}_s^{(1)}, ..., \boldsymbol{\alpha}_s^{(K)})$ and 
        $(\epsilon_{s,w}^{(1)}, ..., \epsilon_{s,w}^{(K)})$. Each of the
        ILI and parameter posterior distribution draws was then plugged into equation
        (6) to produce
        
        \[
         \widetilde{H}_{s,w + i}^{(k)} = \alpha_{0s}^{(k)} + 
                                          \alpha_{1s}^{(k)} 
                                          (\widetilde{ILI}_{s,w +i}^{(k)} \times P) + 
                                          \alpha_{2s}^{(k)} 
                                          (\widetilde{ILI}_{s,w +i}^{(k)} \times P)^2 + 
                                          \phi^{(k)} \widetilde{H}_{s,w + i - 1}^{(k)} + 
                                          \epsilon_{s,w + i}^{(k)}
        \]
        where $k \in \{1, ..., K\}$. The hospitalization draws were then combined into
        $\{H^{(k)}_{s,w + i}\}^K = (\widetilde{H}_{s,w + i}^{(1)}, ..., \widetilde{H}_{s,w + i}^{(K)})$,
        from which
        the probabilistic hospitalization forecast was estimated. In practice we set
        $K = 50,000$, and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        \\~\\
        \indent Occasionally $\{H^{(k)}_{s,w + i}\}^K$ included a small number of negative 
        values,
        which do not make sense when the distribution is meant to forecast
        hospitalizations, a nonnegative number. We attempted to alleviate this problem
        by modeling log-hospitalizations or by assuming hospitalizations followed a
        distribution truncated at 0, but these models led to poorer forecasts
        and issues drawing from the posterior distribution.
        For the forecast competition analysis in 
        section 5, all negative values of $\{H^{(k)}_{s,w + i}\}^K$ 
        were 
        set to 0 to reflect realistic values of hospitalizations and comply with the 
        FluSight forecasting rules.
        \\~\\
    
        \indent In the two-stage scheme, the posterior distribution of 
        model (1) parameters was not directly influenced by the 
        hospitalization data and the posterior distribution of model 
        (6) parameters was not directly influenced by model 
        (1) parameters. Hence there was limited feedback between the
        two components, but propagation of uncertainty in ILI forecasts was still 
        permitted in the estimating of hospitalization forecasts.
        This two-stage modeling is remeniscent of the Bayesian cut 
        described in \cite{plummer2015cuts} and \cite{nott2023bayesian}. Cutting a model
        into two component models or ``modules`` may be justified for several reasons
        including to reduce time to fit the model, avoiding mixing issues of posterior
        sampling distributions, improving predictive performance or to prevent
        model misspecification in one model to influence the other 
        \cite[]{nott2023bayesian, jacob2020unbiased, jacob2017better, 
        lee2017rigorous, plummer2015cuts}. A drawback
        from cutting a model is potential loss of useful information. 
        In our current 
        application for example, the ILI model parameters receive no potentially 
        useful information from hospiatlization data.
        \\~\\
        \indent The decision to cut the forecast model into two component models
        was made for three major reasons.
        When fitting the fully Bayesian joint model, the time to fit a single model 
        using
        MCMC sampling and
        obtain forecasts was sometimes many hours whereas fitting the two models
        separately took a fraction of the time. When assessing convergence of model
        parameters, it was rare that posterior sampling chains for the joint model
        showed convergence even after 55,000 iterations, whereas the component model 
        fits gave no signs which raised concern for convergence. And most importantly,
        the forecasts from the component models generally outperformed those of the 
        joint model in terms of minimizing proper scoring rules.
        \\~\\
  \end{quote}
  \emph{
  \item I recommend including the statement raised by the reviewer in the first 
  submission—that WIS approximates CRPS (ideally supported with a reference)—and 
  therefore leads to essentially the same conclusions. I think the motivation for 
  using WIS instead of CRPS is well explained, especially given that WIS is the 
  primary evaluation measure used in the FluSight competition} 
  \\~\\
  
  The following statements were added to page 17.
  \begin{quote}

      Commonly used proper scoring rules for continuous distribution forecasts
      include the log score and the continuously
      ranked probability score (CRPS).
      \\~\\  

      We note that as the number of forecast intervals $B$ increases, the WIS becomes 
      arbitrarily close to
      the CRPS. Thus, forecast scores under the CRPS and the WIS should be similar
      \cite[]{bracher2021evaluating}.
      \\~\\
\end{quote}

\end{enumerate}



\Huge
\noindent R1
\normalsize
\\~\\
\noindent \emph{The authors have addressed all my comments from the first round. I only have two remaining points
on the extended empirical analysis on FluSight.}

\begin{enumerate}[1.]

  \item \emph{The authors mention the difficulty of missing submissions during the FluSight season (hence the
    display chosen for Figure 10). How was this handled for Table 2? Do the averages refer to different
    sets of forecasting tasks for each model? If so the results are not fully comparable. The referenced
    paper by Cramer et al contains a (heuristic) scheme to compare scores in such a setting, which
    resembles what was done for Figure 10. This may be more appropriate than reporting raw averages.
    Models with very small submission fractions (maybe those with less than 80\%, definitely those with
    less than 50\%) in my opinion should just be removed altogether.}
    \\~\\
          The columns RLWIS and RWIS in table 2 correspond directly to the 
          values plotted in figure 10. We rearranged the table so that RLWIS and
          RWIS come first. The updated table, shown as "Table 1" is below.
          To clarify what's given in table 2 we stated the
          following.
     
    \begin{quote}
      Table 2 gives the numerical values for RLWIS 
      and relative WIS (RWIS) over
      all locations and weeks for each model, relative to the ASGD\_NORM2 model.
      It also includes the non relative mean WIS and LWIS for each model, that is the 
      overall mean scores for all forecasts submitted regardless of submission overlap
      with another model.
    \end{quote}
    
    
    \begin{quote}
    
      During the FluSight competition, most participating teams would occasionally 
      miss a forecast submission for some week, location, or horizon.
      The FluSight 
      models which are used for comparison are the
      13 non-ensemble models which submitted forecasts for at least 80\% of 
      FluSight targets during the 2023 season. 
      
      
      Because forecast submissions for some targets were occassionally missed
      by participating teams, for any two models there was often a mismatch between
      targets for which forecasts were included or exlcuded. This makes a
      simple average of scores is an imperfect measure for comparison.
    \end{quote}
  
  
  \item \emph{It is uncommon to report median scores as their statistical properties are unclear. As mean score
    are also there I'm not fundamentally opposed to reporting them, but in my opinion mean scores
    alone are sufficient.}  
    \\~\\
    Thank you for this comment. Because of it, we removed the median scores
    from the table which improved the aesthetic of the table as well as simplified
    it. See the new table below.
\\~\\    
\end{enumerate}


\begin{table}[ht]
\centering
% \begin{adjustwidth}{-3cm}{-1.5cm}
\caption{Overall summary scores for each of the 13 non-ensemble FluSight
models with over 80\% submissions and 
the ASGD\_NORM2 and SIRD\_NORM2 models.
Summaries include 
mean weighted interval the relative log-weighted interval
and weighted interval scores (RLWIS/RWIS),
the overall mean log-weighted and weighted interval scores (MLWIS/MWIS), 
mean squared error
difference between predictive model empirical and theoretical coverage (MSEC)
see figure 9 c)), and percentage of forecasts 
submitted for all 
targets during the 2023 season (\% Forcs).}
\small
\begin{tabular}{lrrrrrrrr}
  % \hline
                              & RLWIS & RWIS & MLWIS & MWIS & MSEC  & \% Forcs\\ 
  \hline 
  ASGD\_NORM2                 & 1     & 1    & 0.34  & 62   & 0.001 & 100 \\ 
  PSI-PROF                    & 0.98  & 0.98 & 0.35  & 63   & 0.003 & 100 \\ 
  SIRD\_NORM2                 & 0.95  & 0.93 & 0.36  & 67   & 0.001 & 100 \\ 
  CEPH-Rtrend\_fluH           & 0.87  & 0.89 & 0.39  & 69   & 0.004 & 100 \\ 
  FluSight-baseline           & 0.83  & 0.73 & 0.41  & 85   & 0.023 & 100 \\ 
  UM-DeepOutbreak             & 0.74  & 0.73 & 0.46  & 83   & 0.010 & 99 \\ 
  cfarenewal-cfaepimlight     & 0.93  & 0.82 & 0.37  & 76   & 0.001 & 99 \\ 
  MOBS-GLEAM\_FLUH            & 0.97  & 0.95 & 0.35  & 66   & 0.021 & 98 \\ 
  LosAlamos\_NAU-CModel\_Flu  & 0.54  & 0.26 & 0.64  & 242  & 0.153 & 93 \\ 
  UGuelph-CompositeCurve      & 0.66  & 0.50 & 0.52  & 132  & 0.018 & 93 \\ 
  LUcompUncertLab-chimera     & 0.73  & 0.76 & 0.43  & 84   & 0.004 & 92 \\ 
  UGA\_flucast-INFLAenza      & 1.09  & 0.90 & 0.32  & 74   & 0.003 & 90 \\ 
  cfa-flumech                 & 0.86  & 0.65 & 0.41  & 101  & 0.019 & 89 \\ 
  UGA\_flucast-Copycat        & 0.99  & 0.89 & 0.35  & 76   & 0.012 & 87 \\ 
  GT-FluFNP                   & 0.74  & 0.56 & 0.48  & 105  & 0.046 & 86 \\ 
  % UNC\_IDD-InfluPaint         & 107 & 0.35 & 16 & 0.24 & 0.043 & 78 & 0.58 & 0.95 \\ 
  % NU\_UCSD-GLEAM\_AI\_FLUH    & 98 & 0.39 & 22 & 0.24 & 0.014 & 74 & 0.68 & 0.83 \\ 
  % SigSci-CREG                 & 59 & 0.49 & 12 & 0.31 & 0.018 & 71 & 0.62 & 0.74 \\ 
  % PSI-PROF\_beta              & 77 & 0.32 & 18 & 0.20 & 0.004 & 67 & 0.87 & 0.93 \\ 
  % NIH-Flu\_ARIMA              & 230 & 0.34 & 28 & 0.23 & 0.001 & 20 & 0.57 & 0.88 \\ 
  % GH-model                    & 161 & 1.52 & 28 & 1.47 & 0.373 & 17 & 0.18 & 0.27 \\ 
  % UGA\_flucast-OKeeffe        & 27 & 0.59 & 8 & 0.43 & 0.003 & 10 & 0.60 & 0.58 \\  
\end{tabular}
% \end{adjustwidth}
\label{tab:fin_analysis_stats}
\end{table}


\newpage

\Huge
\noindent R2
\normalsize
\\~\\
% Forecasting Influenza Hospitaliztions Using a Bayesian Hierarchical Nonlinear 
% Model with Discrepancy
% 
% 
% This manuscript seems to be a very reasonable attempt at forecasting influenza 
% induced hospitalizations. The
% manuscript also puts the methodology in the context of the FluSight competition 
% organised by the CDC in the US.
% Apologies, I have found it hard to review as it is conceptually very hard to do 
% so in my opinion. Firstly,
% I do not find comfortable modelling the proportion, rather a natural outcome to 
% model will be the number of hospitalization using a Generalized Linear model 
% (Poisson distribution). Secondly, I find it un-intuitive to model the
% flu curve using an Asymmetric Gaussian (ASG) Function. For a statistical model 
% we focus on two aspects: the systematic part and the random part. Here, as far 
% as  I understand the systematic part is the ASG function. The random
% part has several components, which are hidden in Equation (5). My difficulty in 
% modelling using the ASG
% function lies in my inability to relate any meaningful meteorological variables 
% (which are very much drivers of the seasonal flu outbreaks). See for example 
% the paper,  ``A hierarchical Bayesian model for improving short-term forecasting 
% of hospital demand by including meteorological information`` by  SK Sahu, 
% B Baffour, PR Harper, JH Minty, C Sarran
% Journal of the Royal Statistical Society, Series A (2014).

% In spite of the above criticisms, 
\emph{I appreciate the hierarchical modeling 
approach of fitting the two models separately in components. But I wonder how 
the uncertainty has been propagated from one component model to another. 
Cutting the feedback idea is well understood but one still needs to integrate 
uncertainty to get an overall measure of the uncertainty in the forecasts. 
I did not clearly understand this issue from my reading of  the paper. I can 
suggest one way to propagate this uncertainty is by using MCMC samples 
(and methods), see for example, the paper,  ``A rigorous statistical framework 
for spatio-temporal pollution prediction and estimation of its long-term impact 
on health`` by 
D Lee, S Mukhopadhyay, A Rushworth, and SK Sahu, in Biostatistics (2017).}
\\~\\
We appreciated this comment and the included reference. We used the reference
to assist in our new explanation, and made sure to cite it. 
To clarify how uncertainty was propogated, we consildated sections
  3.7 and 3.8, reordered the main ideas, and removed redundant information. We
  also added some advantages/disadvantages of our approach. The changes now
  read:
  \begin{quote}
        \subsection*{3.7 Hospitalization Forecasts Using the Bayesian Cut}
        \label{sec:implementation_posterior}
        
        Rather than combine the ILI and hopsitalization models into a single model,
        (1) and 
        (6) were fit separately via Bayesian posterior updating 
        in two stages and then fused together to produce hospitalization 
        forecasts. In the first stage, future ILI 
        forecasts were obtained from the posterior predictive distribution where for
        week $w$, the predictive distribution was obtained by integrating over the 
        parameters $\pi_s$, $\kappa_s$, $\sigma^2_{\gamma}$, and $\sigma^2_{\gamma_W}$ 
        as in (\ref{eq:ili_post}) where 
        $p(\boldsymbol{\pi}_s, \kappa_s, \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w)$ 
        is the density function of the posterior distribution for the model parameters
        and $\textbf{ILI}_w$ represents all observed ILI data up to week $w$. 
        The desired forecasts were for weeks $w + i$ 
        where $i$ is a positive integer. Here $\widetilde{ILI}_{s,w + i}$ represents the 
        predicted or forecasted ILI at $i$ weeks in the future.
        
        \begin{equation}
            \label{eq:ili_post}
            \tag{7}
            p(\widetilde{ILI}_{s,w +i} | \textbf{ILI}_w) = \int \int \int \int 
            p(\widetilde{ILI}_{s,w + i} | \boldsymbol{\pi}_s, \kappa_s, 
            \sigma^2_{\gamma}, \sigma^2_{\gamma_W}) p(\boldsymbol{\pi}_s, \kappa_s, 
            \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w) 
            d\boldsymbol{\pi}_s d \kappa_s d \sigma^2_{\gamma} d \sigma^2_{\gamma_W}
        \end{equation}
        % \jarad{It doesn't make sense to have 4 $\int$ here since $\pi_s$ is a vector.}
        % \spencer{Isn't that captured in $d \pi_s$?}
        
        \indent In the second stage, the posterior distribution
        for parameters of model (6) given observed
        hospitalization data $\textbf{H}_w$, observed ILI data, and
        denoted as 
        $p(\boldsymbol{\alpha}_s, \sigma_{\epsilon_s} | \textbf{H}_w, \textbf{ILI}_w)$ 
        was obtained.
        The posterior predictive in (\ref{eq:ili_post}) was then combined with 
        the hospitalization parameter posterior distribution
        to produce a hospitalization forecast $\widetilde{H}_{s,w + i}$ 
        for week $w + i$.
        Both the ILI posterior predictive in (\ref{eq:ili_post}) and the hospitalization
        parameter posterior were estimated via Markov chain Monte Carlo (MCMC) sampling.
        That is, the first stage component model produced $K$ draws
        $(\widetilde{ILI}_{s,w +i}^{(1)}, ..., \widetilde{ILI}_{s,w +i}^{(K)})$ 
        and the second stage component model produced
        $K$ parameter draws 
        $(\boldsymbol{\alpha}_s^{(1)}, ..., \boldsymbol{\alpha}_s^{(K)})$ and 
        $(\epsilon_{s,w}^{(1)}, ..., \epsilon_{s,w}^{(K)})$. Each of the
        ILI and parameter posterior distribution draws was then plugged into equation
        (6) to produce
        
        \[
         \widetilde{H}_{s,w + i}^{(k)} = \alpha_{0s}^{(k)} + 
                                          \alpha_{1s}^{(k)} 
                                          (\widetilde{ILI}_{s,w +i}^{(k)} \times P) + 
                                          \alpha_{2s}^{(k)} 
                                          (\widetilde{ILI}_{s,w +i}^{(k)} \times P)^2 + 
                                          \phi^{(k)} \widetilde{H}_{s,w + i - 1}^{(k)} + 
                                          \epsilon_{s,w + i}^{(k)}
        \]
        where $k \in \{1, ..., K\}$. The hospitalization draws were then combined into
        $\{H^{(k)}_{s,w + i}\}^K = (\widetilde{H}_{s,w + i}^{(1)}, ..., \widetilde{H}_{s,w + i}^{(K)})$,
        from which
        the probabilistic hospitalization forecast was estimated. In practice we set
        $K = 50,000$, and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        \\~\\
        \indent Occasionally $\{H^{(k)}_{s,w + i}\}^K$ included a small number of negative 
        values,
        which do not make sense when the distribution is meant to forecast
        hospitalizations, a nonnegative number. We attempted to alleviate this problem
        by modeling log-hospitalizations or by assuming hospitalizations followed a
        distribution truncated at 0, but these models led to poorer forecasts
        and issues drawing from the posterior distribution.
        For the forecast competition analysis in 
        section 5, all negative values of $\{H^{(k)}_{s,w + i}\}^K$ 
        were 
        set to 0 to reflect realistic values of hospitalizations and comply with the 
        FluSight forecasting rules.
        \\~\\
        
        \indent In the two-stage scheme, the posterior distribution of 
        model (1) parameters was not directly influenced by the 
        hospitalization data and the posterior distribution of model 
        (6) parameters was not directly influenced by model 
        (1) parameters. Hence there was limited feedback between the
        two components, but propagation of uncertainty in ILI forecasts was still 
        permitted in the estimating of hospitalization forecasts.
        This two-stage modeling is remeniscent of the Bayesian cut 
        described in \cite{plummer2015cuts} and \cite{nott2023bayesian}. Cutting a model
        into two component models or ``modules`` may be justified for several reasons
        including to reduce time to fit the model, avoiding mixing issues of posterior
        sampling distributions, improving predictive performance or to prevent
        model misspecification in one model to influence the other 
        \cite[]{nott2023bayesian, jacob2020unbiased, jacob2017better, 
        lee2017rigorous, plummer2015cuts}. A drawback
        from cutting a model is potential loss of useful information. 
        In our current 
        application for example, the ILI model parameters receive no potentially 
        useful information from hospiatlization data.
        \\~\\
        \indent The decision to cut the forecast model into two component models
        was made for three major reasons.
        When fitting the fully Bayesian joint model, the time to fit a single model 
        using
        MCMC sampling and
        obtain forecasts was sometimes many hours whereas fitting the two models
        separately took a fraction of the time. When assessing convergence of model
        parameters, it was rare that posterior sampling chains for the joint model
        showed convergence even after 55,000 iterations, whereas the component model 
        fits gave no signs which raised concern for convergence. And most importantly,
        the forecasts from the component models generally outperformed those of the 
        joint model in terms of minimizing proper scoring rules.
        \\~\\
  \end{quote}
  

\emph{In conclusion, I do think that the authors have done a good job of analysis and 
simulation based demonstrations of the results. These can be further improved 
by including some better measures of assessing prediction/forecasting accuracy, 
e.g. the CRPS (Continuous Ranked Probability Score), see for example Chapter 6 
of the book
``Bayesian modeling of spatio-temporal data with R`` by Sahu 
(Chapman and Hall, 2022).} 
\\~\\
In the simulation study in section 4 of the first version of this paper, 
scoring was
done using the CRPS, but in the FluSight analysis in section 5 scoring was done
using the WIS because the available forecasts are not amenable to being scored
the with the CRPS. A recommendation from one of the reviewers was to simplify 
by choosing one or the other scores, and we chose the WIS. To respond to this 
point, however, we modified the first paragraph on page 17. The paragraph is 
below and the added statements are bolded.


\begin{quote}

Model comparison was done by calculating a proper scoring rule for each 
forecast.
Proper scoring rules are the 
current standard for comparing performance between probabilistic forecasts 
and selecting the best forecasts according to the notion of maximizing 
sharpness subject to (auto-)calibration 
\cite[]{gneiting2007probabilistic, tsyplakov2013evaluation}. Proper scoring 
rules are commonly used in forecast comparison, and they are designed such 
that a 
forecaster is incentivized to be honest in the reporting of their forecasts 
\cite[]{gneiting2007strictly, gneiting2014probabilistic}.
\textbf{Commonly used proper scoring rules for continuous distribution forecasts
include the log score and the continuously
ranked probability score (CRPS).} 
The proper scoring rule primarily used in the FluSight competition and the 
COVID-19 Forecast Hub, and which is used in this simulation study,
is the negatively oriented
(smaller is better)
weighted interval score (WIS)
\cite[]{mathis2024evaluation, bracher2021evaluating}.
The WIS is used for scoring quantile or interval 
forecasts \textbf{--the forecast format used in both FluSight and the 
COVID-19 Forecast Hub--}
or forecasts made up of predicitive intervals of several nominal 
levels
\cite[]{gneiting2007strictly, gneiting2014probabilistic, bracher2021evaluating}. 
The WIS is 
defined in (8) where $Q$ is a forecast represented by all 
included quantiles, $B$ is the number of intervals,  $y^*$ is the observed 
value targeted by the forecast, $w_0 = 1/2$ and $w_b = \alpha_b / 2$ are 
weights for each interval, and $\alpha_b$ is the nominal level of the $b^{th}$ 
interval where $b \in \{1, ..., B\}$. 
$IS_{\alpha}$ is the interval score (IS), a proper scoring rule for a 
single interval, defined in (9). Here 
$\vmathbb{1}\{\cdot\}$ is the indicator function.
\textbf{We note that as the number of forecast intervals $B$ increases, the WIS becomes 
arbitrarily close to
the CRPS. Thus, forecast scores under the CRPS and the WIS should be similar
\cite[]{bracher2021evaluating}.}
\end{quote}





\bibliographystyle{ba}
\bibliography{master_bib}

  
\end{document}
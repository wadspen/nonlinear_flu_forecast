\documentclass[ba]{imsart}

\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue,backref=page,backref=page]{hyperref}
\RequirePackage{graphicx}


%%%%%%%%%%%packages added by Spencer%%%%%%%%%%%%%
\usepackage{booktabs} %for the table in the analysis section
\usepackage{enumerate} %for the short algorithm
\usepackage{subfigure} %for making plots with multiple images
\usepackage{tabularx} %for the spacing in tabular
\usepackage{newtxmath} %for making indicator function
\usepackage{changepage} %for adjusting the margins
\usepackage{dsfont} %for getting the indicator function
\usepackage{mathtools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\1}[1]{\mathds{1}\left[#1\right]}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{xcolor}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}
\newcommand{\spencer}[1]{{\color{red} Response: #1}}

\begin{document}
\Huge
\noindent Response to reviewers
\\~\\
\normalsize
We are grateful to the three reviewers and their insightful comments and 
questions. We apologize for the omission of responses to some comments in our
resubmission on 12/5/25, and are embarassed for having somehow overlooked that.
We thank the editor for the chance to correct that mistake, and we have 
corrected it.
\\~\\
Overall, the reviewer comments
made a huge impact on improving the content of
the manuscript. In the most recent revision, we are particularly grateful for 
the questions on uncertainty propagation. Obviously the explanation we gave
for how uncertainty was propagated wasn't clear enough, so in this revision
we reworked sections 3.7 and 3.8 to clarify the procedure.
Specifics are found below and in the 
revised manuscript. Reviewer comments are \emph{italicized}, our responses to 
each reviewer's comments are in normal script, 
and specific changes/additions to the main manuscript are 
indented.
\\~\\


\Huge
\noindent AE1
\normalsize
\\~\\

% \emph{
% I think the authors did a great job in this newly submitted version and 
% addressed most of the earlier raised comments in a nice and convincing way. 
% The paper is well written and interesting to read. 
% }
% I only have two remaining comments.

\begin{enumerate}[-]

  \item \emph{Section 3.7. End of first paragraph: ``..., but propagation of 
  uncertainty   in ILI forecasts was still permitted in the estimating of 
  hospitalisation forecasts.``
  }
  \emph{
  Please clarify more explicitly how uncertainty was propagated between the two 
  model components. My understanding is that this was achieved by incorporating 
  not only point estimates of the parameters but also samples from the predictive 
  distribution of the ILI model. If this is correct, please state it clearly 
  already at this stage and consider adding a brief discussion of the advantages 
  and disadvantages of this approach.}
  \\~\\
  To clarify how uncertainty was propagated, we consolidated sections
  3.7 and 3.8, reordered the main ideas, and removed redundant information. We
  also added some advantages/disadvantages of our approach. The changes now
  read:
  \begin{quote}
        \subsection*{3.7 Hospitalization Forecasts Using the Bayesian Cut}
        \label{sec:implementation_posterior}
        
        Rather than combine the ILI and hospitalization models into a single model,
        forecasts were made via a two-stage fitting process. In the first stage,
        the ILI model (1) and hospitalization model
        (6) were fit separately via Bayesian posterior updating. 
        In the second stage, the two model fits were
        fused together to produce hospitalization 
        forecasts. Future ILI 
        forecasts were obtained from the ILI forecast model in (\ref{eq:ili_post}).
        Here 
        $p(\xi | \textbf{ILI}_w)$ 
        is the posterior distribution of the ILI model (1)
        and $\textbf{ILI}_w$ represents all observed ILI data up to week $w$.
        The ILI forecast model was obtained by integrating over the 
        parameters 
        $\xi = (\pi$, $\kappa$, $\sigma^2_{\gamma}$, $\sigma^2_{\gamma_W})$.
        The desired forecasts are for weeks $w + i$ 
        where $i$ is a positive integer, and $\widetilde{ILI}_{w + i}$ represents the 
        predicted or forecasted ILI at $i$ weeks in the future.
        Note that
        in this section
        the parameter subscript for season $s$ is dropped to simplify notation.
        
        % \begin{equation}
        %     \label{eq:ili_post}
        %     p(\widetilde{ILI}_{s,w +i} | \textbf{ILI}_w) = \int \int \int \int 
        %     p(\widetilde{ILI}_{s,w + i} | \boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W}) p(\boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w) 
        %     d\boldsymbol{\pi}_s d \kappa_s d \sigma^2_{\gamma} d \sigma^2_{\gamma_W}
        % \end{equation}
        
        \begin{equation}
        \tag{7}
            \label{eq:ili_post}
            p(\widetilde{ILI}_{w +i} | \textbf{ILI}_w) = \int  
            p(\widetilde{ILI}_{w + i} | \xi) 
            p(\xi | \textbf{ILI}_w) 
            d\xi
        \end{equation}
        % \jarad{It doesn't make sense to have 4 $\int$ here since $\pi_s$ is a vector.}
        % \spencer{Isn't that captured in $d \pi_s$?}
        Also in the first stage, the posterior distribution of the hospitalization 
        model (6) given observed
        hospitalization data $\textbf{H}_w$ and $\textbf{ILI}_w$ was obtained.
        The posterior distribution is denoted as
        $p(\psi | \textbf{H}_w, \textbf{ILI}_w)$.
        In the second stage,
        the ILI forecast model in (\ref{eq:ili_post}) was then combined with 
        the hospitalization posterior distribution
        to produce a hospitalization forecast $\widetilde{H}_{w + i}$ 
        for week $w + i$.
        
        Both the ILI forecast model in (\ref{eq:ili_post}) and the hospitalization
        posterior distributions were estimated via Markov chain Monte Carlo 
        (MCMC) sampling.
        From the fit ILI model, $K$ MCMC draws
        $(\widetilde{ILI}_{w +i}^{(1)}, ..., \widetilde{ILI}_{w +i}^{(K)})$
        from the forecast model were produced, 
        $K$ draws $(\psi^{(1)}, ..., \psi^{(K)})$
        from the hospitalization model posterior distribution
        were produced. Each of the
        ILI forecast and hospitalization draws was then plugged into 
        the hospitalization model
        (6) to produce the approximate hospitalization forecast
        distribution in the bottom line of (\ref{eq:hosp_post_pred}). 
        This has 
        similarities to the procedure used in section 3.2.2 of 
        \cite[]{sahu2014hierarchical} who use it to propagate the uncertainty of 
        temperature forecasts into forecast model for hospitalizations.
        Here $k \in \{1, ..., K\}$, 
        and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        In the analyses in sections 4 and 5
        we set $K = 50,000$ draws. 
        
        
        \begin{equation}
        \tag{8}
        \begin{aligned}
        \label{eq:hosp_post_pred}
          &p(\widetilde{H}_{w + i} | \textbf{H}_w, \textbf{ILI}_w)\\
          &= \int p(\widetilde{H}_{w +i} | 
               \psi, \widetilde{ILI}_{w +i}, \textbf{H}_w)
               p(\psi | \textbf{H}_w, \textbf{ILI}_w) d \psi d\widetilde{ILI} \\
          &\approx \frac{1}{K} p(\widetilde{H}_{w + i}^{(k)} | \psi^{(k)},  
                              \tilde{ILI}_{w +i}^{(k)}, \textbf{H}_w, \textbf{ILI}_w) 
        \end{aligned}
        \end{equation}
        % \[
        %  \widetilde{H}_{s,w + i}^{(k)} = \alpha_{0s}^{(k)} + 
        %                                   \alpha_{1s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P) + 
        %                                   \alpha_{2s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P)^2 + 
        %                                   \phi^{(k)} \widetilde{H}_{s,w + i - 1}^{(k)} + 
        %                                   \epsilon_{s,w + i}^{(k)}
        % \]
        % where $k \in \{1, ..., K\}$. 
        % The hospitalization draws were then combined into
        % $\{H^{(k)}_{s,w + i}\}^K = (\widetilde{H}_{s,w + i}^{(1)}, ..., \widetilde{H}_{s,w + i}^{(K)})$,
        % from which
        % the probabilistic hospitalization forecast was estimated.
        % In practice we set
        % $K = 50,000$, and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        
        % \[ \begin{array}{rl}
        % p(\tilde{H}_w | H_w, ILI_w) 
        % &=               \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | H_w, ILI_w)  d\tilde{ILI}_w \\
        % &\approx  \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | ILI_w)  d\tilde{ILI}_w
        % \end{array} \]
        
        
        
        In the two-stage scheme, the posterior distribution of the ILI
        model (1) parameters was not directly influenced by the 
        hospitalization data and the posterior distribution of the hospitalization model 
        (6) was not directly influenced by ILI model 
        parameters. Hence there was limited feedback between the
        two components, but the propagation of uncertainty in ILI forecasts was still 
        accomplished in estimating hospitalization forecasts through
        the forecast model (\ref{eq:hosp_post_pred}).
        This two-stage modeling is reminiscent of the Bayesian cut 
        described in \cite[]{plummer2015cuts} and \cite[]{nott2023bayesian}. Cutting a model
        into two component models or ``modules'' may be justified for several reasons
        including to reduce time to fit the model, avoiding mixing issues of posterior
        sampling distributions, improving predictive performance or to prevent
        model misspecification in one model to influence the other 
        \cite[]{nott2023bayesian, jacob2020unbiased, jacob2017better, 
        lee2017rigorous, plummer2015cuts}. A drawback
        from cutting a model is potential loss of useful information. 
        In our current 
        application for example, the ILI model parameters receive no potentially 
        useful information from hospitalization data.
        
        The decision to cut the forecast model into two component models
        was made for three major reasons.
        When fitting the fully Bayesian joint model, the time to fit a single model 
        using
        MCMC sampling and
        obtain forecasts was sometimes many hours whereas fitting the two models
        separately took a fraction of the time. When assessing convergence of model
        parameters, it was rare that posterior sampling chains for the joint model
        showed convergence even after 55,000 iterations, whereas the component model 
        fits gave no signs which raised concerns about convergence. And most importantly,
        the forecasts from the component models generally outperformed those of the 
        joint model in terms of minimizing proper scoring rules.
        \\~\\
  \end{quote}
  \emph{
  \item I recommend including the statement raised by the reviewer in the first 
  submission—that WIS approximates CRPS (ideally supported with a reference)—and 
  therefore leads to essentially the same conclusions. I think the motivation for 
  using WIS instead of CRPS is well explained, especially given that WIS is the 
  primary evaluation measure used in the FluSight competition} 
  \\~\\
  
  The following statements were added on page 17.
  \begin{quote}

      Commonly used proper scoring rules for continuous distribution forecasts
      include the log score and the continuously
      ranked probability score (CRPS).
      \\~\\  

      We note that as the number of forecast intervals $B$ increases, the WIS becomes 
      arbitrarily close to
      the CRPS. Thus, forecast scores under the CRPS and the WIS should be similar
      \cite[]{bracher2021evaluating}.
      \\~\\
\end{quote}

\end{enumerate}



\Huge
\noindent R1
\normalsize
\\~\\
\noindent \emph{The authors have addressed all my comments from the first round. I only have two remaining points
on the extended empirical analysis on FluSight.}

\begin{enumerate}[1.]

  \item \emph{The authors mention the difficulty of missing submissions during the FluSight season (hence the
    display chosen for Figure 10). How was this handled for Table 2? Do the averages refer to different
    sets of forecasting tasks for each model? If so the results are not fully comparable. The referenced
    paper by Cramer et al contains a (heuristic) scheme to compare scores in such a setting, which
    resembles what was done for Figure 10. This may be more appropriate than reporting raw averages.
    Models with very small submission fractions (maybe those with less than 80\%, definitely those with
    less than 50\%) in my opinion should just be removed altogether.}
    \\~\\
          The columns RLWIS and RWIS in table 2 correspond to the 
          values plotted in figure 10, except that in the figure the scores
          are faceted by region or week. 
          We rearranged the table so that RLWIS and
          RWIS come first. The updated table, shown as "Table 1" below.
          To clarify what's given in table 2 we stated the
          following on page 22
     
    \begin{quote}
      Table 2 gives the numerical values for RLWIS 
      and relative WIS (RWIS) over
      all locations and weeks for each model, relative to the ASGD\_NORM2 model.
      It also includes the non relative mean WIS and LWIS for each model, that is the 
      overall mean scores for all forecasts submitted regardless of submission overlap
      with another model.
    \end{quote}
    
    We also removed comparisons with any competing models with less than 
    80\% submissions and further clarified the need to use relative scores.
    The following 2 snippets were added on page 20.
    
    \begin{quote}
    
      During the FluSight competition, most participating teams would occasionally 
      miss a forecast submission for some week, location, or horizon.
      The FluSight 
      models which are used for comparison are the
      13 non-ensemble models which submitted forecasts for at least 80\% of 
      FluSight targets during the 2023 season. 
      \\~\\
      
      Because forecast submissions for some targets were occasionally missed
      by participating teams, for any two models there was often a mismatch between
      targets for which forecasts were included or excluded. This makes a
      simple average of scores an imperfect measure for comparison.
    \end{quote}
  
  
  \item \emph{It is uncommon to report median scores as their statistical properties are unclear. As mean score
    are also there I'm not fundamentally opposed to reporting them, but in my opinion mean scores
    alone are sufficient.}  
    \\~\\
    Thank you for this comment. Because of it, we removed the median scores
    from the table. This simplified the table and also greatly 
    improved the aesthetic when compared to the previous table. 
    See the new table below.
\\~\\    
\end{enumerate}


\begin{table}[ht]
\centering
% \begin{adjustwidth}{-3cm}{-1.5cm}
\caption{Overall summary scores for each of the 13 non-ensemble FluSight
models with over 80\% submissions and 
the ASGD\_NORM2 and SIRD\_NORM2 models.
Summaries include 
mean weighted interval the relative log-weighted interval
and weighted interval scores (RLWIS/RWIS),
the overall mean log-weighted and weighted interval scores (MLWIS/MWIS), 
mean squared error
difference between predictive model empirical and theoretical coverage (MSEC)
see figure 9 c)), and percentage of forecasts 
submitted for all 
targets during the 2023 season (\% Forcs).}
\small
\begin{tabular}{lrrrrrrrr}
  % \hline
                              & RLWIS & RWIS & MLWIS & MWIS & MSEC  & \% Forcs\\ 
  \hline 
  ASGD\_NORM2                 & 1     & 1    & 0.34  & 62   & 0.001 & 100 \\ 
  PSI-PROF                    & 0.98  & 0.98 & 0.35  & 63   & 0.003 & 100 \\ 
  SIRD\_NORM2                 & 0.95  & 0.93 & 0.36  & 67   & 0.001 & 100 \\ 
  CEPH-Rtrend\_fluH           & 0.87  & 0.89 & 0.39  & 69   & 0.004 & 100 \\ 
  FluSight-baseline           & 0.83  & 0.73 & 0.41  & 85   & 0.023 & 100 \\ 
  UM-DeepOutbreak             & 0.74  & 0.73 & 0.46  & 83   & 0.010 & 99 \\ 
  cfarenewal-cfaepimlight     & 0.93  & 0.82 & 0.37  & 76   & 0.001 & 99 \\ 
  MOBS-GLEAM\_FLUH            & 0.97  & 0.95 & 0.35  & 66   & 0.021 & 98 \\ 
  LosAlamos\_NAU-CModel\_Flu  & 0.54  & 0.26 & 0.64  & 242  & 0.153 & 93 \\ 
  UGuelph-CompositeCurve      & 0.66  & 0.50 & 0.52  & 132  & 0.018 & 93 \\ 
  LUcompUncertLab-chimera     & 0.73  & 0.76 & 0.43  & 84   & 0.004 & 92 \\ 
  UGA\_flucast-INFLAenza      & 1.09  & 0.90 & 0.32  & 74   & 0.003 & 90 \\ 
  cfa-flumech                 & 0.86  & 0.65 & 0.41  & 101  & 0.019 & 89 \\ 
  UGA\_flucast-Copycat        & 0.99  & 0.89 & 0.35  & 76   & 0.012 & 87 \\ 
  GT-FluFNP                   & 0.74  & 0.56 & 0.48  & 105  & 0.046 & 86 \\ 
  % UNC\_IDD-InfluPaint         & 107 & 0.35 & 16 & 0.24 & 0.043 & 78 & 0.58 & 0.95 \\ 
  % NU\_UCSD-GLEAM\_AI\_FLUH    & 98 & 0.39 & 22 & 0.24 & 0.014 & 74 & 0.68 & 0.83 \\ 
  % SigSci-CREG                 & 59 & 0.49 & 12 & 0.31 & 0.018 & 71 & 0.62 & 0.74 \\ 
  % PSI-PROF\_beta              & 77 & 0.32 & 18 & 0.20 & 0.004 & 67 & 0.87 & 0.93 \\ 
  % NIH-Flu\_ARIMA              & 230 & 0.34 & 28 & 0.23 & 0.001 & 20 & 0.57 & 0.88 \\ 
  % GH-model                    & 161 & 1.52 & 28 & 1.47 & 0.373 & 17 & 0.18 & 0.27 \\ 
  % UGA\_flucast-OKeeffe        & 27 & 0.59 & 8 & 0.43 & 0.003 & 10 & 0.60 & 0.58 \\  
\end{tabular}
% \end{adjustwidth}
\label{tab:fin_analysis_stats}
\end{table}


\newpage

\Huge
\noindent R2
\normalsize
\\~\\
% Forecasting Influenza Hospitaliztions Using a Bayesian Hierarchical Nonlinear 
% Model with Discrepancy
% 
% 
\emph{This manuscript seems to be a very reasonable attempt at forecasting influenza
induced hospitalizations. The
manuscript also puts the methodology in the context of the FluSight competition
organised by the CDC in the US.
Apologies, I have found it hard to review as it is conceptually very hard to do
so in my opinion.}

\emph{Firstly,
I do not find comfortable modelling the proportion, rather a natural outcome to
model will be the number of hospitalization using a Generalized Linear model
(Poisson distribution).} 

The influenza like illness (ILI) data model in equation (1) is indeed 
proportion data, and is reported as the proportion of patients visiting
a provider who display flu like symptoms. This is stated in the introduction
on page 2, but in the revision we restate it again as below on page 4

\begin{quote}
An ILI case is defined as a 
``fever (temperature of $100^{\circ}$F[$37.8^{\circ}$C] or greater) and a cough 
and/or a sore throat,'' and ILI data is reported as the proportion 
or percentage of patients who display these symptoms.
\end{quote}

For that reason, we argue that 
model (1) is appropriate for ILI data. 
To clarify that the purpose of this paper is
to use historically accurate ILI forecasts as a predictor of hospitalizations,
we added the following two statements; the first in the introduction on page
3 and the second in section 3 on page 7.

\begin{quote}
The contribution of this manuscript is to introduce a two component 
Bayesian modeling framework 
for modeling HHS hospitalization forecasts where hospitalization data and 
years of ILI data are used to inform forecast models. 
As is shown in this manuscript,
ILI is a good predictor of hospitalizations, 
and years of experience have led
to effective ILI forecast methods. Thus producing accurate ILI forecasts 
and using them to inform hospitalizations greatly assists in producing
accurate hospitalization forecasts.
\end{quote}

\begin{quote}
Presented in this section is methodology for forecasting hospitalizations
using ILI forecasts as a predictor of hospitalization forecasts. 
The availability of nearly a decade of ILI data along with accurate forecast
models allows us to make accurate forecasts of ILI which are used as a linear
predictor for hospitalization forecasts.
\end{quote}


Hospitalization data which we are trying to forecast is
count data. Thus a Poisson is certainly a natural modeling choice. To 
keep the model simple, however, we modeled hospitalizations using a normal
distribution, a location-scale t distribution,
or log transformed hospitalizations and modeled the transformed data with
a normal linear model. See model (6).
This choice was made because the forecasts were accurate,
the modeling is simpler, and fitting the model using a Poisson was much slower
making it more difficult to meet the forecast deadlines. This argument was 
added on page 12.

\begin{quote}
Another natural choice would have been to
model hospitalizations with a Poisson response generalized linear model. 
However, a generalized linear model adds additional complexity and we found that
fitting the model required significantly more time than
model (6) making it more difficult to produce forecasts
for all 54 locations and meet the FluSight 
submission deadline which was roughly 12 hours after data were released.
Also to avoid complexity, the authors of \cite[]{sahu2014hierarchical} opted to 
forecast hospitalizations using a normal response model rather than a Poisson
response model.
\end{quote}



\emph{Secondly, I find it un-intuitive to model the
flu curve using an Asymmetric Gaussian (ASG) Function. For a statistical model
we focus on two aspects: the systematic part and the random part. Here, as far
as  I understand the systematic part is the ASG function. The random
part has several components, which are hidden in Equation (5). My difficulty in
modelling using the ASG
function lies in my inability to relate any meaningful meteorological variables
(which are very much drivers of the seasonal flu outbreaks). See for example
the paper,  ``A hierarchical Bayesian model for improving short-term forecasting
of hospital demand by including meteorological information`` by  SK Sahu,
B Baffour, PR Harper, JH Minty, C Sarran
Journal of the Royal Statistical Society, Series A (2014).}

The ASG function, and likewise the susceptible-infectious-recovered (SIR)
mechanistic model, are used to model
ILI data because of known annual trajectory, the relationship to 
mathematical infection disease models, and most importantly
the success they've had in 
producing accurate ILI forecasts.
ILI data and forecasts are used as 
linear predictors for hospitalizations in model (6).
Neither model (1) nor (6) preclude the use of additional linear predictors
for prediciting ILI or hospitalizations, but if additional predictors 
were to be used it would make more sense to add them to model (6).
We added the following two statements to discuss this important point.
The first is near the beginning of section 3.5 on page 12, and the second is 
in the discussion on page 24.

\begin{quote}
In model (6), the data is limited to the ILI and 
hospitalization data provided by FluSight, but including additional
linear predictors would be straightforward. In fact it has been shown that 
meteorological predictors and forecasts such as temperature can lead
to improved forecasts as well as geographic variables such as patient
age and sex \cite[]{sahu2014hierarchical}.  
\end{quote}

\begin{quote}
Additionally, it has been shown that meteorological variables
such as local temperature forecasts and geographic variables such as age can 
lead to improved hospitalization forecasts \cite[]{sahu2014hierarchical}. 
A straightforward and useful extension of the methodology herein would be to 
collect such variables and incorporate them as added hospitalization 
predictors.
\end{quote}

% In spite of the above criticisms, 
\emph{I appreciate the hierarchical modeling 
approach of fitting the two models separately in components. But I wonder how 
the uncertainty has been propagated from one component model to another. 
Cutting the feedback idea is well understood but one still needs to integrate 
uncertainty to get an overall measure of the uncertainty in the forecasts. 
I did not clearly understand this issue from my reading of  the paper. I can 
suggest one way to propagate this uncertainty is by using MCMC samples 
(and methods), see for example, the paper,  ``A rigorous statistical framework 
for spatio-temporal pollution prediction and estimation of its long-term impact 
on health`` by 
D Lee, S Mukhopadhyay, A Rushworth, and SK Sahu, in Biostatistics (2017).}
\\~\\
We appreciated this comment and the included reference. We used the reference
to assist in our new explanation, and made sure to cite it. 
To clarify how uncertainty was propagated, we consolidated sections
  3.7 and 3.8, reordered the main ideas, and removed redundant information. We
  also added some advantages/disadvantages of our approach. The changes now
  read:
  \begin{quote}
        \subsection*{3.7 Hospitalization Forecasts Using the Bayesian Cut}
        \label{sec:implementation_posterior}
        
        Rather than combine the ILI and hospitalization models into a single model,
        forecasts were made via a two-stage fitting process. In the first stage,
        the ILI model (1) and hospitalization model
        (6) were fit separately via Bayesian posterior updating. 
        In the second stage, the two model fits were
        fused together to produce hospitalization 
        forecasts. Future ILI 
        forecasts were obtained from the ILI forecast model in (\ref{eq:ili_post}).
        Here 
        $p(\xi | \textbf{ILI}_w)$ 
        is the posterior distribution of the ILI model (1)
        and $\textbf{ILI}_w$ represents all observed ILI data up to week $w$.
        The ILI forecast model was obtained by integrating over the 
        parameters 
        $\xi = (\pi$, $\kappa$, $\sigma^2_{\gamma}$, $\sigma^2_{\gamma_W})$.
        The desired forecasts are for weeks $w + i$ 
        where $i$ is a positive integer, and $\widetilde{ILI}_{w + i}$ represents the 
        predicted or forecasted ILI at $i$ weeks in the future.
        Note that
        in this section
        the parameter subscript for season $s$ is dropped to simplify notation.
        
        % \begin{equation}
        %     \label{eq:ili_post}
        %     p(\widetilde{ILI}_{s,w +i} | \textbf{ILI}_w) = \int \int \int \int 
        %     p(\widetilde{ILI}_{s,w + i} | \boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W}) p(\boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w) 
        %     d\boldsymbol{\pi}_s d \kappa_s d \sigma^2_{\gamma} d \sigma^2_{\gamma_W}
        % \end{equation}
        
        \begin{equation}
        \tag{7}
            \label{eq:ili_post}
            p(\widetilde{ILI}_{w +i} | \textbf{ILI}_w) = \int  
            p(\widetilde{ILI}_{w + i} | \xi) 
            p(\xi | \textbf{ILI}_w) 
            d\xi
        \end{equation}
        % \jarad{It doesn't make sense to have 4 $\int$ here since $\pi_s$ is a vector.}
        % \spencer{Isn't that captured in $d \pi_s$?}
        Also in the first stage, the posterior distribution of the hospitalization 
        model (6) given observed
        hospitalization data $\textbf{H}_w$ and $\textbf{ILI}_w$ was obtained.
        The posterior distribution is denoted as
        $p(\psi | \textbf{H}_w, \textbf{ILI}_w)$.
        In the second stage,
        the ILI forecast model in (\ref{eq:ili_post}) was then combined with 
        the hospitalization posterior distribution
        to produce a hospitalization forecast $\widetilde{H}_{w + i}$ 
        for week $w + i$.
        
        Both the ILI forecast model in (\ref{eq:ili_post}) and the hospitalization
        posterior distributions were estimated via Markov chain Monte Carlo 
        (MCMC) sampling.
        From the fit ILI model, $K$ MCMC draws
        $(\widetilde{ILI}_{w +i}^{(1)}, ..., \widetilde{ILI}_{w +i}^{(K)})$
        from the forecast model were produced, 
        $K$ draws $(\psi^{(1)}, ..., \psi^{(K)})$
        from the hospitalization model posterior distribution
        were produced. Each of the
        ILI forecast and hospitalization draws was then plugged into 
        the hospitalization model
        (6) to produce the approximate hospitalization forecast
        distribution in the bottom line of (\ref{eq:hosp_post_pred}). 
        This has 
        similarities to the procedure used in section 3.2.2 of 
        \cite[]{sahu2014hierarchical}, which uses it to 
        propagate the uncertainty of 
        temperature forecasts into forecast model for hospitalizations.
        Here $k \in \{1, ..., K\}$, 
        and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        In the analyses in sections 4 and 5
        we set $K = 50,000$ draws. 
        
        
        \begin{equation}
        \tag{8}
        \begin{aligned}
        \label{eq:hosp_post_pred}
          &p(\widetilde{H}_{w + i} | \textbf{H}_w, \textbf{ILI}_w)\\
          &= \int p(\widetilde{H}_{w +i} | 
               \psi, \widetilde{ILI}_{w +i}, \textbf{H}_w)
               p(\psi | \textbf{H}_w, \textbf{ILI}_w) d \psi d\widetilde{ILI} \\
          &\approx \frac{1}{K} p(\widetilde{H}_{w + i}^{(k)} | \psi^{(k)},  
                              \tilde{ILI}_{w +i}^{(k)}, \textbf{H}_w, \textbf{ILI}_w) 
        \end{aligned}
        \end{equation}
        % \[
        %  \widetilde{H}_{s,w + i}^{(k)} = \alpha_{0s}^{(k)} + 
        %                                   \alpha_{1s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P) + 
        %                                   \alpha_{2s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P)^2 + 
        %                                   \phi^{(k)} \widetilde{H}_{s,w + i - 1}^{(k)} + 
        %                                   \epsilon_{s,w + i}^{(k)}
        % \]
        % where $k \in \{1, ..., K\}$. 
        % The hospitalization draws were then combined into
        % $\{H^{(k)}_{s,w + i}\}^K = (\widetilde{H}_{s,w + i}^{(1)}, ..., \widetilde{H}_{s,w + i}^{(K)})$,
        % from which
        % the probabilistic hospitalization forecast was estimated.
        % In practice we set
        % $K = 50,000$, and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        
        % \[ \begin{array}{rl}
        % p(\tilde{H}_w | H_w, ILI_w) 
        % &=               \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | H_w, ILI_w)  d\tilde{ILI}_w \\
        % &\approx  \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | ILI_w)  d\tilde{ILI}_w
        % \end{array} \]
        
        
        
        In the two-stage scheme, the posterior distribution of the ILI
        model (1) parameters was not directly influenced by the 
        hospitalization data and the posterior distribution of the hospitalization model 
        (6) was not directly influenced by ILI model 
        parameters. Hence there was limited feedback between the
        two components, but the propagation of uncertainty in ILI forecasts was still 
        accomplished in estimating hospitalization forecasts through
        the forecast model (\ref{eq:hosp_post_pred}).
        This two-stage modeling is reminiscent of the Bayesian cut 
        described in \cite[]{plummer2015cuts} and \cite[]{nott2023bayesian}. Cutting a model
        into two component models or ``modules'' may be justified for several reasons
        including to reduce time to fit the model, avoiding mixing issues of posterior
        sampling distributions, improving predictive performance or to prevent
        model misspecification in one model to influence the other 
        \cite[]{nott2023bayesian, jacob2020unbiased, jacob2017better, 
        lee2017rigorous, plummer2015cuts}. A drawback
        from cutting a model is potential loss of useful information. 
        In our current 
        application for example, the ILI model parameters receive no potentially 
        useful information from hospitalization data.
        
        The decision to cut the forecast model into two component models
        was made for three major reasons.
        When fitting the fully Bayesian joint model, the time to fit a single model 
        using
        MCMC sampling and
        obtain forecasts was sometimes many hours whereas fitting the two models
        separately took a fraction of the time. When assessing convergence of model
        parameters, it was rare that posterior sampling chains for the joint model
        showed convergence even after 55,000 iterations, whereas the component model 
        fits gave no signs which raised concerns about convergence. And most importantly,
        the forecasts from the component models generally outperformed those of the 
        joint model in terms of minimizing proper scoring rules.
        \\~\\
  \end{quote}
  

\emph{In conclusion, I do think that the authors have done a good job of analysis and 
simulation based demonstrations of the results. These can be further improved 
by including some better measures of assessing prediction/forecasting accuracy, 
e.g. the CRPS (Continuous Ranked Probability Score), see for example Chapter 6 
of the book
``Bayesian modeling of spatio-temporal data with R`` by Sahu 
(Chapman and Hall, 2022).} 
\\~\\
In the simulation study in section 4 of the first version of this paper, 
scoring was
done using the CRPS, but in the FluSight analysis in section 5 scoring was done
using the WIS because the available forecasts are not amenable to being scored
the with the CRPS. A recommendation from one of the reviewers led us to simplify 
by choosing one or the other scores, and we chose the WIS.
In response to your comment, 
we modified the first paragraph on page 17. The paragraph is 
below and the added statements are in bold.


\begin{quote}

Model comparison was done by calculating a proper scoring rule for each 
forecast.
Proper scoring rules are the 
current standard for comparing performance between probabilistic forecasts 
and selecting the best forecasts according to the notion of maximizing 
sharpness subject to (auto-)calibration 
\cite[]{gneiting2007probabilistic, tsyplakov2013evaluation}. Proper scoring 
rules are commonly used in forecast comparison, and they are designed such 
that a 
forecaster is incentivized to be honest in the reporting of their forecasts 
\cite[]{gneiting2007strictly, gneiting2014probabilistic}.
\textbf{Commonly used proper scoring rules for continuous distribution forecasts
include the log score and the continuously
ranked probability score (CRPS).} 
The proper scoring rule primarily used in the FluSight competition and the 
COVID-19 Forecast Hub, and which is used in this simulation study,
is the negatively oriented
(smaller is better)
weighted interval score (WIS)
\cite[]{mathis2024evaluation, bracher2021evaluating}.
The WIS is used for scoring quantile or interval 
forecasts \textbf{--the forecast format used in both FluSight and the 
COVID-19 Forecast Hub--}
or forecasts made up of predictive intervals of several nominal 
levels
\cite[]{gneiting2007strictly, gneiting2014probabilistic, bracher2021evaluating}. 
The WIS is 
defined in (8) where $Q$ is a forecast represented by all 
included quantiles, $B$ is the number of intervals,  $y^*$ is the observed 
value targeted by the forecast, $w_0 = 1/2$ and $w_b = \alpha_b / 2$ are 
weights for each interval, and $\alpha_b$ is the nominal level of the $b^{th}$ 
interval where $b \in \{1, ..., B\}$. 
$IS_{\alpha}$ is the interval score (IS), a proper scoring rule for a 
single interval, defined in (9). Here 
$\vmathbb{1}\{\cdot\}$ is the indicator function.
\textbf{We note that as the number of forecast intervals $B$ increases, the WIS becomes 
arbitrarily close to
the CRPS. Thus, forecast scores under the CRPS and the WIS should be similar
\cite[]{bracher2021evaluating}.}
\end{quote}





\bibliographystyle{ba}
\bibliography{master_bib}

  
\end{document}
\documentclass[ba]{imsart}

\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}%% uncomment this for author-year citations
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue,backref=page,backref=page]{hyperref}
\RequirePackage{graphicx}


%%%%%%%%%%%packages added by Spencer%%%%%%%%%%%%%
\usepackage{booktabs} %for the table in the analysis section
\usepackage{enumerate} %for the short algorithm
\usepackage{subfigure} %for making plots with multiple images
\usepackage{tabularx} %for the spacing in tabular
\usepackage{newtxmath} %for making indicator function
\usepackage{changepage} %for adjusting the margins
\usepackage{dsfont} %for getting the indicator function
\usepackage{mathtools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\1}[1]{\mathds{1}\left[#1\right]}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{xcolor}
\newcommand{\jarad}[1]{{\color{red} Jarad: #1}}
\newcommand{\spencer}[1]{{\color{red} Response: #1}}

\begin{document}
\Huge
\noindent Response to reviewers
\\~\\
\normalsize
We are grateful to the three reviewers and their insightful comments and 
questions. Overall, they made a huge difference in improving the content of
the manuscript. In the most recent revision, we are particularly grateful for 
the questions on uncertainty propogation. Obviously the explanation we gave
for how uncertainty was propogated wasn't clear enough, so in this iteration of 
we reworked sections 3.7 and 3.8 to make more clear how 
it was done. Specifics are found below and in the 
revised manuscript. Reviewer comments are \emph{italicized}, our responses to 
each reviewer's comments are in normal script, 
and specific changes/additions to the main manuscript are 
indented.
\\~\\


\Huge
\noindent AE1
\normalsize
\\~\\

% \emph{
% I think the authors did a great job in this newly submitted version and 
% addressed most of the earlier raised comments in a nice and convincing way. 
% The paper is well written and interesting to read. 
% }
% I only have two remaining comments.

\begin{enumerate}[-]

  \item \emph{Section 3.7. End of of first paragraph: ``..., but propagation of 
  uncertainty   in ILI forecasts was still permitted in the estimating of 
  hospitalisation forecasts.``
  }
  \emph{
  Please clarify more explicitly how uncertainty was propagated between the two 
  model components. My understanding is that this was achieved by incorporating 
  not only point estimates of the parameters but also samples from the predictive 
  distribution of the ILI model. If this is correct, please state it clearly 
  already at this stage and consider adding a brief discussion of the advantages 
  and disadvantages of this approach.}
  \\~\\
  To clarify how uncertainty was propogated, we consolidated sections
  3.7 and 3.8, reordered the main ideas, and removed redundant information. We
  also added some advantages/disadvantages of our approach. The changes now
  read:
  \begin{quote}
        \subsection{3.7 Hospitalization Forecasts Using the Bayesian Cut}
        \label{sec:implementation_posterior}
        
        Rather than combine the ILI and hopsitalization models into a single model,
        forecasts were made via a two-stage fitting process. In the first stage,
        the ILI model (1) and hospitalization model
        (6) were fit separately via Bayesian posterior updating. 
        In the second stage, the two model fits were
        fused together to produce hospitalization 
        forecasts. Future ILI 
        forecasts were obtained from the ILI forecast model in (\ref{eq:ili_post}).
        Here 
        $p(\xi | \textbf{ILI}_w)$ 
        is the posterior distribution of the ILI model (1)
        and $\textbf{ILI}_w$ represents all observed ILI data up to week $w$.
        The ILI forecast model was obtained by integrating over the 
        parameters 
        $\xi = (\pi$, $\kappa$, $\sigma^2_{\gamma}$, $\sigma^2_{\gamma_W})$.
        The desired forecasts are for weeks $w + i$ 
        where $i$ is a positive integer, and $\widetilde{ILI}_{w + i}$ represents the 
        predicted or forecasted ILI at $i$ weeks in the future.
        Note that
        in this section
        the parameter subscript for season $s$ is dropped to simplify notation.
        
        % \begin{equation}
        %     \label{eq:ili_post}
        %     p(\widetilde{ILI}_{s,w +i} | \textbf{ILI}_w) = \int \int \int \int 
        %     p(\widetilde{ILI}_{s,w + i} | \boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W}) p(\boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w) 
        %     d\boldsymbol{\pi}_s d \kappa_s d \sigma^2_{\gamma} d \sigma^2_{\gamma_W}
        % \end{equation}
        
        \begin{equation}
        \tag{7}
            \label{eq:ili_post}
            p(\widetilde{ILI}_{w +i} | \textbf{ILI}_w) = \int  
            p(\widetilde{ILI}_{w + i} | \xi) 
            p(\xi | \textbf{ILI}_w) 
            d\xi
        \end{equation}
        % \jarad{It doesn't make sense to have 4 $\int$ here since $\pi_s$ is a vector.}
        % \spencer{Isn't that captured in $d \pi_s$?}
        Also in the first stage, the posterior distribution of the hospitalization 
        model (6) given observed
        hospitalization data $\textbf{H}_w$ and $\textbf{ILI}_w$ was obtained.
        The posterior distribution is denoted as
        $p(\psi | \textbf{H}_w, \textbf{ILI}_w)$.
        In the second stage,
        the ILI forecast model in (\ref{eq:ili_post}) was then combined with 
        the hospitalization posterior distribution
        to produce a hospitalization forecast $\widetilde{H}_{w + i}$ 
        for week $w + i$.
        
        Both the ILI forecast model in (\ref{eq:ili_post}) and the hospitalization
        posterior distributions were estimated via Markov chain Monte Carlo 
        (MCMC) sampling.
        From the fit ILI model, $K$ MCMC draws
        $(\widetilde{ILI}_{w +i}^{(1)}, ..., \widetilde{ILI}_{w +i}^{(K)})$
        from the forecast model were produced, 
        $K$ draws $(\psi^{(1)}, ..., \psi^{(K)})$
        from the hospitalization model posterior distribution
        were produced. Each of the
        ILI forecast and hospitalization draws was then plugged into 
        the hospitalization model
        (6) to produce the approximate hospitalization forecast
        distribution in the bottom line of (\ref{eq:hosp_post_pred}). 
        Here $k \in \{1, ..., K\}$, 
        and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        In the analyses in sections 4 and 5
        we set $K = 50,000$ draws. 
        
        
        \begin{equation}
        \tag{8}
        \begin{aligned}
        \label{eq:hosp_post_pred}
          &p(\widetilde{H}_{w + i} | \textbf{H}_w, \textbf{ILI}_w)\\
          &= \int p(\widetilde{H}_{w +i} | 
               \psi, \widetilde{ILI}_{w +i}, \textbf{H}_w)
               p(\psi | \textbf{H}_w, \textbf{ILI}_w) d \psi d\widetilde{ILI} \\
          &\approx \frac{1}{K} p(\widetilde{H}_{w + i}^{(k)} | \psi^{(k)},  
                              \tilde{ILI}_{w +i}^{(k)}, \textbf{H}_w, \textbf{ILI}_w) 
        \end{aligned}
        \end{equation}
        % \[
        %  \widetilde{H}_{s,w + i}^{(k)} = \alpha_{0s}^{(k)} + 
        %                                   \alpha_{1s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P) + 
        %                                   \alpha_{2s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P)^2 + 
        %                                   \phi^{(k)} \widetilde{H}_{s,w + i - 1}^{(k)} + 
        %                                   \epsilon_{s,w + i}^{(k)}
        % \]
        % where $k \in \{1, ..., K\}$. 
        % The hospitalization draws were then combined into
        % $\{H^{(k)}_{s,w + i}\}^K = (\widetilde{H}_{s,w + i}^{(1)}, ..., \widetilde{H}_{s,w + i}^{(K)})$,
        % from which
        % the probabilistic hospitalization forecast was estimated.
        % In practice we set
        % $K = 50,000$, and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        
        % \[ \begin{array}{rl}
        % p(\tilde{H}_w | H_w, ILI_w) 
        % &=               \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | H_w, ILI_w)  d\tilde{ILI}_w \\
        % &\approx  \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | ILI_w)  d\tilde{ILI}_w
        % \end{array} \]
        
        
        
        In the two-stage scheme, the posterior distribution of the ILI
        model (1) parameters was not directly influenced by the 
        hospitalization data and the posterior distribution of the hospitalization model 
        (6) was not directly influenced by ILI model 
        parameters. Hence there was limited feedback between the
        two components, but the propagation of uncertainty in ILI forecasts was still 
        accomplished in the estimating of hospitalization forecasts through
        the forecast model (\ref{eq:hosp_post_pred}).
        This two-stage modeling is reminiscent of the Bayesian cut 
        described in \cite{plummer2015cuts} and \cite{nott2023bayesian}. Cutting a model
        into two component models or ``modules'' may be justified for several reasons
        including to reduce time to fit the model, avoiding mixing issues of posterior
        sampling distributions, improving predictive performance or to prevent
        model misspecification in one model to influence the other 
        \cite[]{nott2023bayesian, jacob2020unbiased, jacob2017better, 
        lee2017rigorous, plummer2015cuts}. A drawback
        from cutting a model is potential loss of useful information. 
        In our current 
        application for example, the ILI model parameters receive no potentially 
        useful information from hospiatlization data.
        
        The decision to cut the forecast model into two component models
        was made for three major reasons.
        When fitting the fully Bayesian joint model, the time to fit a single model 
        using
        MCMC sampling and
        obtain forecasts was sometimes many hours whereas fitting the two models
        separately took a fraction of the time. When assessing convergence of model
        parameters, it was rare that posterior sampling chains for the joint model
        showed convergence even after 55,000 iterations, whereas the component model 
        fits gave no signs which raised concern about convergence. And most importantly,
        the forecasts from the component models generally outperformed those of the 
        joint model in terms of minimizing proper scoring rules.
        \\~\\
  \end{quote}
  \emph{
  \item I recommend including the statement raised by the reviewer in the first 
  submission—that WIS approximates CRPS (ideally supported with a reference)—and 
  therefore leads to essentially the same conclusions. I think the motivation for 
  using WIS instead of CRPS is well explained, especially given that WIS is the 
  primary evaluation measure used in the FluSight competition} 
  \\~\\
  
  The following statements were added to page 17.
  \begin{quote}

      Commonly used proper scoring rules for continuous distribution forecasts
      include the log score and the continuously
      ranked probability score (CRPS).
      \\~\\  

      We note that as the number of forecast intervals $B$ increases, the WIS becomes 
      arbitrarily close to
      the CRPS. Thus, forecast scores under the CRPS and the WIS should be similar
      \cite[]{bracher2021evaluating}.
      \\~\\
\end{quote}

\end{enumerate}



\Huge
\noindent R1
\normalsize
\\~\\
\noindent \emph{The authors have addressed all my comments from the first round. I only have two remaining points
on the extended empirical analysis on FluSight.}

\begin{enumerate}[1.]

  \item \emph{The authors mention the difficulty of missing submissions during the FluSight season (hence the
    display chosen for Figure 10). How was this handled for Table 2? Do the averages refer to different
    sets of forecasting tasks for each model? If so the results are not fully comparable. The referenced
    paper by Cramer et al contains a (heuristic) scheme to compare scores in such a setting, which
    resembles what was done for Figure 10. This may be more appropriate than reporting raw averages.
    Models with very small submission fractions (maybe those with less than 80\%, definitely those with
    less than 50\%) in my opinion should just be removed altogether.}
    \\~\\
          The columns RLWIS and RWIS in table 2 correspond to the 
          values plotted in figure 10, except that in the figure the scores
          are faceted by region or week. 
          We rearranged the table so that RLWIS and
          RWIS come first. The updated table, shown as "Table 1" is below.
          To clarify what's given in table 2 we stated the
          following on page 22
     
    \begin{quote}
      Table 2 gives the numerical values for RLWIS 
      and relative WIS (RWIS) over
      all locations and weeks for each model, relative to the ASGD\_NORM2 model.
      It also includes the non relative mean WIS and LWIS for each model, that is the 
      overall mean scores for all forecasts submitted regardless of submission overlap
      with another model.
    \end{quote}
    
    We also removed comparisons with any competing models with less than 
    80\% submissions and further clarified the need to use relative scores.
    The following 2 snippets were added on page 20.
    
    \begin{quote}
    
      During the FluSight competition, most participating teams would occasionally 
      miss a forecast submission for some week, location, or horizon.
      The FluSight 
      models which are used for comparison are the
      13 non-ensemble models which submitted forecasts for at least 80\% of 
      FluSight targets during the 2023 season. 
      \\~\\
      
      Because forecast submissions for some targets were occassionally missed
      by participating teams, for any two models there was often a mismatch between
      targets for which forecasts were included or exlcuded. This makes a
      simple average of scores is an imperfect measure for comparison.
    \end{quote}
  
  
  \item \emph{It is uncommon to report median scores as their statistical properties are unclear. As mean score
    are also there I'm not fundamentally opposed to reporting them, but in my opinion mean scores
    alone are sufficient.}  
    \\~\\
    Thank you for this comment. Because of it, we removed the median scores
    from the table. This simplified the table and also greatly 
    improved the aesthetic when compared to the previous table. 
    See the new table below.
\\~\\    
\end{enumerate}


\begin{table}[ht]
\centering
% \begin{adjustwidth}{-3cm}{-1.5cm}
\caption{Overall summary scores for each of the 13 non-ensemble FluSight
models with over 80\% submissions and 
the ASGD\_NORM2 and SIRD\_NORM2 models.
Summaries include 
mean weighted interval the relative log-weighted interval
and weighted interval scores (RLWIS/RWIS),
the overall mean log-weighted and weighted interval scores (MLWIS/MWIS), 
mean squared error
difference between predictive model empirical and theoretical coverage (MSEC)
see figure 9 c)), and percentage of forecasts 
submitted for all 
targets during the 2023 season (\% Forcs).}
\small
\begin{tabular}{lrrrrrrrr}
  % \hline
                              & RLWIS & RWIS & MLWIS & MWIS & MSEC  & \% Forcs\\ 
  \hline 
  ASGD\_NORM2                 & 1     & 1    & 0.34  & 62   & 0.001 & 100 \\ 
  PSI-PROF                    & 0.98  & 0.98 & 0.35  & 63   & 0.003 & 100 \\ 
  SIRD\_NORM2                 & 0.95  & 0.93 & 0.36  & 67   & 0.001 & 100 \\ 
  CEPH-Rtrend\_fluH           & 0.87  & 0.89 & 0.39  & 69   & 0.004 & 100 \\ 
  FluSight-baseline           & 0.83  & 0.73 & 0.41  & 85   & 0.023 & 100 \\ 
  UM-DeepOutbreak             & 0.74  & 0.73 & 0.46  & 83   & 0.010 & 99 \\ 
  cfarenewal-cfaepimlight     & 0.93  & 0.82 & 0.37  & 76   & 0.001 & 99 \\ 
  MOBS-GLEAM\_FLUH            & 0.97  & 0.95 & 0.35  & 66   & 0.021 & 98 \\ 
  LosAlamos\_NAU-CModel\_Flu  & 0.54  & 0.26 & 0.64  & 242  & 0.153 & 93 \\ 
  UGuelph-CompositeCurve      & 0.66  & 0.50 & 0.52  & 132  & 0.018 & 93 \\ 
  LUcompUncertLab-chimera     & 0.73  & 0.76 & 0.43  & 84   & 0.004 & 92 \\ 
  UGA\_flucast-INFLAenza      & 1.09  & 0.90 & 0.32  & 74   & 0.003 & 90 \\ 
  cfa-flumech                 & 0.86  & 0.65 & 0.41  & 101  & 0.019 & 89 \\ 
  UGA\_flucast-Copycat        & 0.99  & 0.89 & 0.35  & 76   & 0.012 & 87 \\ 
  GT-FluFNP                   & 0.74  & 0.56 & 0.48  & 105  & 0.046 & 86 \\ 
  % UNC\_IDD-InfluPaint         & 107 & 0.35 & 16 & 0.24 & 0.043 & 78 & 0.58 & 0.95 \\ 
  % NU\_UCSD-GLEAM\_AI\_FLUH    & 98 & 0.39 & 22 & 0.24 & 0.014 & 74 & 0.68 & 0.83 \\ 
  % SigSci-CREG                 & 59 & 0.49 & 12 & 0.31 & 0.018 & 71 & 0.62 & 0.74 \\ 
  % PSI-PROF\_beta              & 77 & 0.32 & 18 & 0.20 & 0.004 & 67 & 0.87 & 0.93 \\ 
  % NIH-Flu\_ARIMA              & 230 & 0.34 & 28 & 0.23 & 0.001 & 20 & 0.57 & 0.88 \\ 
  % GH-model                    & 161 & 1.52 & 28 & 1.47 & 0.373 & 17 & 0.18 & 0.27 \\ 
  % UGA\_flucast-OKeeffe        & 27 & 0.59 & 8 & 0.43 & 0.003 & 10 & 0.60 & 0.58 \\  
\end{tabular}
% \end{adjustwidth}
\label{tab:fin_analysis_stats}
\end{table}


\newpage

\Huge
\noindent R2
\normalsize
\\~\\
% Forecasting Influenza Hospitaliztions Using a Bayesian Hierarchical Nonlinear 
% Model with Discrepancy
% 
% 
% This manuscript seems to be a very reasonable attempt at forecasting influenza 
% induced hospitalizations. The
% manuscript also puts the methodology in the context of the FluSight competition 
% organised by the CDC in the US.
% Apologies, I have found it hard to review as it is conceptually very hard to do 
% so in my opinion. Firstly,
% I do not find comfortable modelling the proportion, rather a natural outcome to 
% model will be the number of hospitalization using a Generalized Linear model 
% (Poisson distribution). Secondly, I find it un-intuitive to model the
% flu curve using an Asymmetric Gaussian (ASG) Function. For a statistical model 
% we focus on two aspects: the systematic part and the random part. Here, as far 
% as  I understand the systematic part is the ASG function. The random
% part has several components, which are hidden in Equation (5). My difficulty in 
% modelling using the ASG
% function lies in my inability to relate any meaningful meteorological variables 
% (which are very much drivers of the seasonal flu outbreaks). See for example 
% the paper,  ``A hierarchical Bayesian model for improving short-term forecasting 
% of hospital demand by including meteorological information`` by  SK Sahu, 
% B Baffour, PR Harper, JH Minty, C Sarran
% Journal of the Royal Statistical Society, Series A (2014).

% In spite of the above criticisms, 
\emph{I appreciate the hierarchical modeling 
approach of fitting the two models separately in components. But I wonder how 
the uncertainty has been propagated from one component model to another. 
Cutting the feedback idea is well understood but one still needs to integrate 
uncertainty to get an overall measure of the uncertainty in the forecasts. 
I did not clearly understand this issue from my reading of  the paper. I can 
suggest one way to propagate this uncertainty is by using MCMC samples 
(and methods), see for example, the paper,  ``A rigorous statistical framework 
for spatio-temporal pollution prediction and estimation of its long-term impact 
on health`` by 
D Lee, S Mukhopadhyay, A Rushworth, and SK Sahu, in Biostatistics (2017).}
\\~\\
We appreciated this comment and the included reference. We used the reference
to assist in our new explanation, and made sure to cite it. 
To clarify how uncertainty was propogated, we consildated sections
  3.7 and 3.8, reordered the main ideas, and removed redundant information. We
  also added some advantages/disadvantages of our approach. The changes now
  read:
  \begin{quote}
        \subsection{3.7 Hospitalization Forecasts Using the Bayesian Cut}
        \label{sec:implementation_posterior}
        
        Rather than combine the ILI and hopsitalization models into a single model,
        forecasts were made via a two-stage fitting process. In the first stage,
        the ILI model (1) and hospitalization model
        (6) were fit separately via Bayesian posterior updating. 
        In the second stage, the two model fits were
        fused together to produce hospitalization 
        forecasts. Future ILI 
        forecasts were obtained from the ILI forecast model in (\ref{eq:ili_post}).
        Here 
        $p(\xi | \textbf{ILI}_w)$ 
        is the posterior distribution of the ILI model (1)
        and $\textbf{ILI}_w$ represents all observed ILI data up to week $w$.
        The ILI forecast model was obtained by integrating over the 
        parameters 
        $\xi = (\pi$, $\kappa$, $\sigma^2_{\gamma}$, $\sigma^2_{\gamma_W})$.
        The desired forecasts are for weeks $w + i$ 
        where $i$ is a positive integer, and $\widetilde{ILI}_{w + i}$ represents the 
        predicted or forecasted ILI at $i$ weeks in the future.
        Note that
        in this section
        the parameter subscript for season $s$ is dropped to simplify notation.
        
        % \begin{equation}
        %     \label{eq:ili_post}
        %     p(\widetilde{ILI}_{s,w +i} | \textbf{ILI}_w) = \int \int \int \int 
        %     p(\widetilde{ILI}_{s,w + i} | \boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W}) p(\boldsymbol{\pi}_s, \kappa_s, 
        %     \sigma^2_{\gamma}, \sigma^2_{\gamma_W} | \textbf{ILI}_w) 
        %     d\boldsymbol{\pi}_s d \kappa_s d \sigma^2_{\gamma} d \sigma^2_{\gamma_W}
        % \end{equation}
        
        \begin{equation}
        \tag{7}
            \label{eq:ili_post}
            p(\widetilde{ILI}_{w +i} | \textbf{ILI}_w) = \int  
            p(\widetilde{ILI}_{w + i} | \xi) 
            p(\xi | \textbf{ILI}_w) 
            d\xi
        \end{equation}
        % \jarad{It doesn't make sense to have 4 $\int$ here since $\pi_s$ is a vector.}
        % \spencer{Isn't that captured in $d \pi_s$?}
        Also in the first stage, the posterior distribution of the hospitalization 
        model (6) given observed
        hospitalization data $\textbf{H}_w$ and $\textbf{ILI}_w$ was obtained.
        The posterior distribution is denoted as
        $p(\psi | \textbf{H}_w, \textbf{ILI}_w)$.
        In the second stage,
        the ILI forecast model in (\ref{eq:ili_post}) was then combined with 
        the hospitalization posterior distribution
        to produce a hospitalization forecast $\widetilde{H}_{w + i}$ 
        for week $w + i$.
        
        Both the ILI forecast model in (\ref{eq:ili_post}) and the hospitalization
        posterior distributions were estimated via Markov chain Monte Carlo 
        (MCMC) sampling.
        From the fit ILI model, $K$ MCMC draws
        $(\widetilde{ILI}_{w +i}^{(1)}, ..., \widetilde{ILI}_{w +i}^{(K)})$
        from the forecast model were produced, 
        $K$ draws $(\psi^{(1)}, ..., \psi^{(K)})$
        from the hospitalization model posterior distribution
        were produced. Each of the
        ILI forecast and hospitalization draws was then plugged into 
        the hospitalization model
        (6) to produce the approximate hospitalization forecast
        distribution in the bottom line of (\ref{eq:hosp_post_pred}). 
        Here $k \in \{1, ..., K\}$, 
        and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        In the analyses in sections 4 and 5
        we set $K = 50,000$ draws. 
        
        
        \begin{equation}
        \tag{8}
        \begin{aligned}
        \label{eq:hosp_post_pred}
          &p(\widetilde{H}_{w + i} | \textbf{H}_w, \textbf{ILI}_w)\\
          &= \int p(\widetilde{H}_{w +i} | 
               \psi, \widetilde{ILI}_{w +i}, \textbf{H}_w)
               p(\psi | \textbf{H}_w, \textbf{ILI}_w) d \psi d\widetilde{ILI} \\
          &\approx \frac{1}{K} p(\widetilde{H}_{w + i}^{(k)} | \psi^{(k)},  
                              \tilde{ILI}_{w +i}^{(k)}, \textbf{H}_w, \textbf{ILI}_w) 
        \end{aligned}
        \end{equation}
        % \[
        %  \widetilde{H}_{s,w + i}^{(k)} = \alpha_{0s}^{(k)} + 
        %                                   \alpha_{1s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P) + 
        %                                   \alpha_{2s}^{(k)} 
        %                                   (\widetilde{ILI}_{s,w +i}^{(k)} \times P)^2 + 
        %                                   \phi^{(k)} \widetilde{H}_{s,w + i - 1}^{(k)} + 
        %                                   \epsilon_{s,w + i}^{(k)}
        % \]
        % where $k \in \{1, ..., K\}$. 
        % The hospitalization draws were then combined into
        % $\{H^{(k)}_{s,w + i}\}^K = (\widetilde{H}_{s,w + i}^{(1)}, ..., \widetilde{H}_{s,w + i}^{(K)})$,
        % from which
        % the probabilistic hospitalization forecast was estimated.
        % In practice we set
        % $K = 50,000$, and forecasts were obtained for weeks $i \in \{1, 2, 3, 4\}$.
        
        % \[ \begin{array}{rl}
        % p(\tilde{H}_w | H_w, ILI_w) 
        % &=               \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | H_w, ILI_w)  d\tilde{ILI}_w \\
        % &\approx  \int p(\tilde{H}_w | \tilde{ILI}_w, H_w) p (\tilde{ILI}_w | ILI_w)  d\tilde{ILI}_w
        % \end{array} \]
        
        
        
        In the two-stage scheme, the posterior distribution of the ILI
        model (1) parameters was not directly influenced by the 
        hospitalization data and the posterior distribution of the hospitalization model 
        (6) was not directly influenced by ILI model 
        parameters. Hence there was limited feedback between the
        two components, but the propagation of uncertainty in ILI forecasts was still 
        accomplished in the estimating of hospitalization forecasts through
        the forecast model (\ref{eq:hosp_post_pred}).
        This two-stage modeling is reminiscent of the Bayesian cut 
        described in \cite{plummer2015cuts} and \cite{nott2023bayesian}. Cutting a model
        into two component models or ``modules'' may be justified for several reasons
        including to reduce time to fit the model, avoiding mixing issues of posterior
        sampling distributions, improving predictive performance or to prevent
        model misspecification in one model to influence the other 
        \cite[]{nott2023bayesian, jacob2020unbiased, jacob2017better, 
        lee2017rigorous, plummer2015cuts}. A drawback
        from cutting a model is potential loss of useful information. 
        In our current 
        application for example, the ILI model parameters receive no potentially 
        useful information from hospiatlization data.
        
        The decision to cut the forecast model into two component models
        was made for three major reasons.
        When fitting the fully Bayesian joint model, the time to fit a single model 
        using
        MCMC sampling and
        obtain forecasts was sometimes many hours whereas fitting the two models
        separately took a fraction of the time. When assessing convergence of model
        parameters, it was rare that posterior sampling chains for the joint model
        showed convergence even after 55,000 iterations, whereas the component model 
        fits gave no signs which raised concern about convergence. And most importantly,
        the forecasts from the component models generally outperformed those of the 
        joint model in terms of minimizing proper scoring rules.
        \\~\\
  \end{quote}
  

\emph{In conclusion, I do think that the authors have done a good job of analysis and 
simulation based demonstrations of the results. These can be further improved 
by including some better measures of assessing prediction/forecasting accuracy, 
e.g. the CRPS (Continuous Ranked Probability Score), see for example Chapter 6 
of the book
``Bayesian modeling of spatio-temporal data with R`` by Sahu 
(Chapman and Hall, 2022).} 
\\~\\
In the simulation study in section 4 of the first version of this paper, 
scoring was
done using the CRPS, but in the FluSight analysis in section 5 scoring was done
using the WIS because the available forecasts are not amenable to being scored
the with the CRPS. A recommendation from one of the reviewers led us to simplify 
by choosing one or the other scores, and we chose the WIS.
In response to your comment, 
we modified the first paragraph on page 17. The paragraph is 
below and the added statements are in bold.


\begin{quote}

Model comparison was done by calculating a proper scoring rule for each 
forecast.
Proper scoring rules are the 
current standard for comparing performance between probabilistic forecasts 
and selecting the best forecasts according to the notion of maximizing 
sharpness subject to (auto-)calibration 
\cite[]{gneiting2007probabilistic, tsyplakov2013evaluation}. Proper scoring 
rules are commonly used in forecast comparison, and they are designed such 
that a 
forecaster is incentivized to be honest in the reporting of their forecasts 
\cite[]{gneiting2007strictly, gneiting2014probabilistic}.
\textbf{Commonly used proper scoring rules for continuous distribution forecasts
include the log score and the continuously
ranked probability score (CRPS).} 
The proper scoring rule primarily used in the FluSight competition and the 
COVID-19 Forecast Hub, and which is used in this simulation study,
is the negatively oriented
(smaller is better)
weighted interval score (WIS)
\cite[]{mathis2024evaluation, bracher2021evaluating}.
The WIS is used for scoring quantile or interval 
forecasts \textbf{--the forecast format used in both FluSight and the 
COVID-19 Forecast Hub--}
or forecasts made up of predicitive intervals of several nominal 
levels
\cite[]{gneiting2007strictly, gneiting2014probabilistic, bracher2021evaluating}. 
The WIS is 
defined in (8) where $Q$ is a forecast represented by all 
included quantiles, $B$ is the number of intervals,  $y^*$ is the observed 
value targeted by the forecast, $w_0 = 1/2$ and $w_b = \alpha_b / 2$ are 
weights for each interval, and $\alpha_b$ is the nominal level of the $b^{th}$ 
interval where $b \in \{1, ..., B\}$. 
$IS_{\alpha}$ is the interval score (IS), a proper scoring rule for a 
single interval, defined in (9). Here 
$\vmathbb{1}\{\cdot\}$ is the indicator function.
\textbf{We note that as the number of forecast intervals $B$ increases, the WIS becomes 
arbitrarily close to
the CRPS. Thus, forecast scores under the CRPS and the WIS should be similar
\cite[]{bracher2021evaluating}.}
\end{quote}





\bibliographystyle{ba}
\bibliography{master_bib}

  
\end{document}